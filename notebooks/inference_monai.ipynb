{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os \n",
    "class Args(argparse.Namespace):\n",
    "    # data=\"/scratch/guest187/Data/val_SSA/monai\"\n",
    "    # data = \"/Users/alexandrasmith/Desktop/Workspace/Projects/UNN_BraTS23/data/val_SSA/monai/\"\n",
    "    data=\"C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\CC\\\\Backup_2407\\\\val_SSA\\\\monai\\\\\"\n",
    "    preproc_set=\"val\"\n",
    "    data_used=\"SSA\"\n",
    "    # results='/Users/alexandrasmith/Desktop/Workspace/Projects/UNN_BraTS23/data/val_SSA/results/'\n",
    "    # results='/scratch/guest187/Data/val_SSA/results/monai_test/'\n",
    "    results='C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\CC\\\\Backup_2407\\\\val_SSA\\\\results\\\\monai_test\\\\'\n",
    "    optimiser=\"adam\"\n",
    "    criterion=\"dice\"\n",
    "    exec_mode=\"predict\"\n",
    "    seed=42\n",
    "    batch_size=4\n",
    "    val_batch_size=2\n",
    "    # ckpt_path='/scratch/guest187/Data/train_all/results/test_fullRunThrough/best_metric_model_fullTest.pth'\n",
    "    # ckpt_path='/Users/alexandrasmith/Desktop/Workspace/Projects/UNN_BraTS23/data/best_metric_model_fullTest.pth'\n",
    "    ckpt_path='C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\CC\\\\Backup_2407\\\\Results\\\\train_all_monai\\\\test_fullRunThrough\\\\best_metric_model_fullTest.pth'\n",
    "    model=\"unet\"\n",
    "args=Args()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB!!!! IF CALLING THE 'FROM monai_functions IMPORT (list of fx)': MUST COPY ABOVE ARGS INTO THE MONAI FX PY SCRIPT AND COMMENT OUT DL.GET ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "# sys.path.append('/Users/alexandrasmith/Desktop/Workspace/Projects/UNN_BraTS23/scripts')\n",
    "sys.path.append('C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\UNN_BraTS23\\\\scripts')\n",
    "import subprocess\n",
    "try:\n",
    "    import os\n",
    "    import numpy as np \n",
    "    import matplotlib.pyplot as plt\n",
    "    import torch\n",
    "    import torchio as tio\n",
    "    import nibabel as nib\n",
    "    from glob import glob\n",
    "    from subprocess import call\n",
    "    from scipy.ndimage import label\n",
    "    from monai.inferers import sliding_window_inference\n",
    "    import modelZoo_monai as mZoo\n",
    "    import data_loader as dl\n",
    "    from monai_functions import (\n",
    "        save_checkpoint,\n",
    "        define_model,\n",
    "        define_dataloaders,\n",
    "        model_params,\n",
    "        val_params,\n",
    "        train)\n",
    "    \n",
    "except ModuleNotFoundError as e:\n",
    "    package = str(e).split(\"'\")[0]\n",
    "    subprocess.run(['pip', 'install', package])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup: Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define inference method\n",
    "def inference(VAL_AMP, model, input):\n",
    "    def _compute(input):\n",
    "        '''\n",
    "        roi_size â€“ the spatial window size for inferences. \n",
    "        When its components have None or non-positives, the corresponding inputs dimension will be used. \n",
    "        if the components of the roi_size are non-positive values, the transform will use the corresponding components of img size.\n",
    "        For example, roi_size=(32, -1) will be adapted to (32, 64) if the second spatial dimension size of img is 64\n",
    "        '''\n",
    "        return sliding_window_inference(\n",
    "            inputs=input,\n",
    "            roi_size=None,\n",
    "            sw_batch_size=4, #sw_batch_size denotes the max number of windows per network inference iteration, not the batch size of inputs.\n",
    "            predictor=model,\n",
    "            overlap=0.2,\n",
    "            mode='constant'\n",
    "        )\n",
    "    if VAL_AMP:\n",
    "        with torch.cuda.amp.autocast():\n",
    "            return _compute(input)\n",
    "    else:\n",
    "        return _compute(input)\n",
    "\n",
    "# Save to nifti\n",
    "def save_nifti(subj, img, outfile):\n",
    "    '''\n",
    "    Save numpy to NIfTI format\n",
    "    '''\n",
    "    image = subj[\"image\"].data\n",
    "    img = nib.load(img) ## load original image from path\n",
    "    nib.save(\n",
    "        nib.Nifti1Image(image, img.affine, header=img.header),\n",
    "        outfile\n",
    "    )\n",
    "\n",
    "def infer():\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(val_loader):\n",
    "            input = batch[0]\n",
    "            # case = str(batch[1]).split('/')[-1].replace(\"-stk.npy',)\", \"\")\n",
    "            case = str(batch[1]).split('\\\\')[-1].replace(\"-stk.npy',)\", \"\")\n",
    "            print(f\"Working on subject: {case}; shape size is: {input.shape}\")\n",
    "            #Run inference\n",
    "            output = inference(VAL_AMP, model, input)\n",
    "            print(f\"Output before post_transforms is: {output.shape}\")\n",
    "            val_output = post_transforms(output[0])\n",
    "            print(f\"Output after post_transforms is: {val_output.shape}\")\n",
    "\n",
    "            #Attempt to convert and save to numpy for post processing\n",
    "            val_output2 = val_output.cpu().numpy()\n",
    "            val_out_pth = os.path.join(args.results, f\"{case}_preds.npy\")\n",
    "            try:\n",
    "                print(f\"trying to save to numpy file, with shape: {val_output2.shape}\")\n",
    "                np.save(val_out_pth, val_output2)\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while saving the file: {e}\")\n",
    "            \n",
    "            #convert tensor to tio subject for padding and affine alignment\n",
    "            subject = tio.Subject(\n",
    "                    image=tio.ScalarImage(tensor=val_output),\n",
    "                    name=case\n",
    "                    )\n",
    "            tranformed_subject = tio.CropOrPad((240, 240, 155))\n",
    "            tranformed_subject = tranformed_subject(subject)\n",
    "            orig_img = os.path.join(args.data, case, f\"{case}-stk.nii.gz\")\n",
    "            outfile = os.path.join(args.results, f\"{case}.nii.gz\")\n",
    "\n",
    "            save_nifti(tranformed_subject, orig_img,outfile )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Processing\n",
    "Add code below once it works :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Inference\n",
    "### Saves initial prediction as .npy, and final prediction after label conversion as .nii.gz with tensor size 240, 240, 155\n",
    "\n",
    "**NOTE: MONAI_testfullrun appears to have used cropping of 192, 192, 128 --> NEED TO SEE WHEN AND WHY THIS WAS CHANGED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args._get_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amoda\\Documents\\SPARK\\BraTS2023\\CC\\Backup_2407\\val_SSA\\monai\\BraTS-SSA-00126-000\n",
      "C:\\Users\\amoda\\Documents\\SPARK\\BraTS2023\\CC\\Backup_2407\\val_SSA\\monai\\BraTS-SSA-00129-000\n",
      "C:\\Users\\amoda\\Documents\\SPARK\\BraTS2023\\CC\\Backup_2407\\val_SSA\\monai\\BraTS-SSA-00132-000\n",
      "C:\\Users\\amoda\\Documents\\SPARK\\BraTS2023\\CC\\Backup_2407\\val_SSA\\monai\\BraTS-SSA-00139-000\n",
      "C:\\Users\\amoda\\Documents\\SPARK\\BraTS2023\\CC\\Backup_2407\\val_SSA\\monai\\BraTS-SSA-00143-000\n",
      "C:\\Users\\amoda\\Documents\\SPARK\\BraTS2023\\CC\\Backup_2407\\val_SSA\\monai\\BraTS-SSA-00148-000\n",
      "C:\\Users\\amoda\\Documents\\SPARK\\BraTS2023\\CC\\Backup_2407\\val_SSA\\monai\\BraTS-SSA-00158-000\n",
      "C:\\Users\\amoda\\Documents\\SPARK\\BraTS2023\\CC\\Backup_2407\\val_SSA\\monai\\BraTS-SSA-00169-000\n",
      "C:\\Users\\amoda\\Documents\\SPARK\\BraTS2023\\CC\\Backup_2407\\val_SSA\\monai\\BraTS-SSA-00180-000\n",
      "C:\\Users\\amoda\\Documents\\SPARK\\BraTS2023\\CC\\Backup_2407\\val_SSA\\monai\\BraTS-SSA-00188-000\n",
      "C:\\Users\\amoda\\Documents\\SPARK\\BraTS2023\\CC\\Backup_2407\\val_SSA\\monai\\BraTS-SSA-00192-000\n",
      "C:\\Users\\amoda\\Documents\\SPARK\\BraTS2023\\CC\\Backup_2407\\val_SSA\\monai\\BraTS-SSA-00198-000\n",
      "C:\\Users\\amoda\\Documents\\SPARK\\BraTS2023\\CC\\Backup_2407\\val_SSA\\monai\\BraTS-SSA-00210-000\n",
      "C:\\Users\\amoda\\Documents\\SPARK\\BraTS2023\\CC\\Backup_2407\\val_SSA\\monai\\BraTS-SSA-00218-000\n",
      "C:\\Users\\amoda\\Documents\\SPARK\\BraTS2023\\CC\\Backup_2407\\val_SSA\\monai\\BraTS-SSA-00227-000\n",
      "C:\\Users\\amoda\\Documents\\SPARK\\BraTS2023\\CC\\Backup_2407\\val_SSA\\monai\\dataset.json\n",
      "Tranformed_subject:  Subject(Keys: ('image', 'name'); images: 1)\n",
      "Working on subject: BraTS-SSA-00126-000; shape size is: torch.Size([1, 4, 192, 192, 128])\n",
      "Output before post_transforms is: torch.Size([1, 4, 192, 192, 128])\n",
      "Output after post_transforms is: torch.Size([4, 192, 192, 128])\n",
      "trying to save to numpy file, with shape: (4, 192, 192, 128)\n",
      "Tranformed_subject:  Subject(Keys: ('image', 'name'); images: 1)\n",
      "Working on subject: BraTS-SSA-00129-000; shape size is: torch.Size([1, 4, 192, 192, 128])\n",
      "Output before post_transforms is: torch.Size([1, 4, 192, 192, 128])\n",
      "Output after post_transforms is: torch.Size([4, 192, 192, 128])\n",
      "trying to save to numpy file, with shape: (4, 192, 192, 128)\n",
      "Tranformed_subject:  Subject(Keys: ('image', 'name'); images: 1)\n",
      "Working on subject: BraTS-SSA-00132-000; shape size is: torch.Size([1, 4, 192, 192, 128])\n",
      "Output before post_transforms is: torch.Size([1, 4, 192, 192, 128])\n",
      "Output after post_transforms is: torch.Size([4, 192, 192, 128])\n",
      "trying to save to numpy file, with shape: (4, 192, 192, 128)\n",
      "Tranformed_subject:  Subject(Keys: ('image', 'name'); images: 1)\n",
      "Working on subject: BraTS-SSA-00139-000; shape size is: torch.Size([1, 4, 192, 192, 128])\n",
      "Output before post_transforms is: torch.Size([1, 4, 192, 192, 128])\n",
      "Output after post_transforms is: torch.Size([4, 192, 192, 128])\n",
      "trying to save to numpy file, with shape: (4, 192, 192, 128)\n",
      "Tranformed_subject:  Subject(Keys: ('image', 'name'); images: 1)\n",
      "Working on subject: BraTS-SSA-00143-000; shape size is: torch.Size([1, 4, 192, 192, 128])\n",
      "Output before post_transforms is: torch.Size([1, 4, 192, 192, 128])\n",
      "Output after post_transforms is: torch.Size([4, 192, 192, 128])\n",
      "trying to save to numpy file, with shape: (4, 192, 192, 128)\n",
      "Tranformed_subject:  Subject(Keys: ('image', 'name'); images: 1)\n",
      "Working on subject: BraTS-SSA-00148-000; shape size is: torch.Size([1, 4, 192, 192, 128])\n",
      "Output before post_transforms is: torch.Size([1, 4, 192, 192, 128])\n",
      "Output after post_transforms is: torch.Size([4, 192, 192, 128])\n",
      "trying to save to numpy file, with shape: (4, 192, 192, 128)\n",
      "Tranformed_subject:  Subject(Keys: ('image', 'name'); images: 1)\n",
      "Working on subject: BraTS-SSA-00158-000; shape size is: torch.Size([1, 4, 192, 192, 128])\n",
      "Output before post_transforms is: torch.Size([1, 4, 192, 192, 128])\n",
      "Output after post_transforms is: torch.Size([4, 192, 192, 128])\n",
      "trying to save to numpy file, with shape: (4, 192, 192, 128)\n",
      "Tranformed_subject:  Subject(Keys: ('image', 'name'); images: 1)\n",
      "Working on subject: BraTS-SSA-00169-000; shape size is: torch.Size([1, 4, 192, 192, 128])\n",
      "Output before post_transforms is: torch.Size([1, 4, 192, 192, 128])\n",
      "Output after post_transforms is: torch.Size([4, 192, 192, 128])\n",
      "trying to save to numpy file, with shape: (4, 192, 192, 128)\n",
      "Tranformed_subject:  Subject(Keys: ('image', 'name'); images: 1)\n",
      "Working on subject: BraTS-SSA-00180-000; shape size is: torch.Size([1, 4, 192, 192, 128])\n",
      "Output before post_transforms is: torch.Size([1, 4, 192, 192, 128])\n",
      "Output after post_transforms is: torch.Size([4, 192, 192, 128])\n",
      "trying to save to numpy file, with shape: (4, 192, 192, 128)\n",
      "Tranformed_subject:  Subject(Keys: ('image', 'name'); images: 1)\n",
      "Working on subject: BraTS-SSA-00188-000; shape size is: torch.Size([1, 4, 192, 192, 128])\n",
      "Output before post_transforms is: torch.Size([1, 4, 192, 192, 128])\n",
      "Output after post_transforms is: torch.Size([4, 192, 192, 128])\n",
      "trying to save to numpy file, with shape: (4, 192, 192, 128)\n",
      "Tranformed_subject:  Subject(Keys: ('image', 'name'); images: 1)\n",
      "Working on subject: BraTS-SSA-00192-000; shape size is: torch.Size([1, 4, 192, 192, 128])\n",
      "Output before post_transforms is: torch.Size([1, 4, 192, 192, 128])\n",
      "Output after post_transforms is: torch.Size([4, 192, 192, 128])\n",
      "trying to save to numpy file, with shape: (4, 192, 192, 128)\n",
      "Tranformed_subject:  Subject(Keys: ('image', 'name'); images: 1)\n",
      "Working on subject: BraTS-SSA-00198-000; shape size is: torch.Size([1, 4, 192, 192, 128])\n",
      "Output before post_transforms is: torch.Size([1, 4, 192, 192, 128])\n",
      "Output after post_transforms is: torch.Size([4, 192, 192, 128])\n",
      "trying to save to numpy file, with shape: (4, 192, 192, 128)\n",
      "Tranformed_subject:  Subject(Keys: ('image', 'name'); images: 1)\n",
      "Working on subject: BraTS-SSA-00210-000; shape size is: torch.Size([1, 4, 192, 192, 128])\n",
      "Output before post_transforms is: torch.Size([1, 4, 192, 192, 128])\n",
      "Output after post_transforms is: torch.Size([4, 192, 192, 128])\n",
      "trying to save to numpy file, with shape: (4, 192, 192, 128)\n",
      "Tranformed_subject:  Subject(Keys: ('image', 'name'); images: 1)\n",
      "Working on subject: BraTS-SSA-00218-000; shape size is: torch.Size([1, 4, 192, 192, 128])\n",
      "Output before post_transforms is: torch.Size([1, 4, 192, 192, 128])\n",
      "Output after post_transforms is: torch.Size([4, 192, 192, 128])\n",
      "trying to save to numpy file, with shape: (4, 192, 192, 128)\n",
      "Tranformed_subject:  Subject(Keys: ('image', 'name'); images: 1)\n",
      "Working on subject: BraTS-SSA-00227-000; shape size is: torch.Size([1, 4, 192, 192, 128])\n",
      "Output before post_transforms is: torch.Size([1, 4, 192, 192, 128])\n",
      "Output after post_transforms is: torch.Size([4, 192, 192, 128])\n",
      "trying to save to numpy file, with shape: (4, 192, 192, 128)\n"
     ]
    }
   ],
   "source": [
    "validation_dir=args.data\n",
    "validation_files = [os.path.join(validation_dir, file) for file in os.listdir(validation_dir)]\n",
    "print('\\n'.join(validation_files))\n",
    "\n",
    "model, n_channels = define_model(args.ckpt_path)\n",
    "\n",
    "# Load validation data to dataloader\n",
    "data_transforms = dl.define_transforms(n_channels)\n",
    "dataloader = dl.load_data(args, data_transforms)\n",
    "\n",
    "val_loader = dataloader['val']\n",
    "\n",
    "# Validation parameters\n",
    "VAL_AMP, dice_metric, dice_metric_batch, post_transforms = val_params()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "infer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check saved nifty is of size 240, 240, 155"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 240, 240, 155)\n"
     ]
    }
   ],
   "source": [
    "sample = nib.load(os.path.join (args.results, \"BraTS-SSA-00126-000.nii.gz\"))\n",
    "print(sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------\n",
    "## FOR ALEX: PLEASE TRY FIGURE OUT HOW TO MAKE THIS WORK FOR OUR DATA WITH LABELS 0, 1, 2, 3/\n",
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post processing copied from OptiNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lbl(pred):\n",
    "    enh = pred[2]\n",
    "    c1, c2, c3 = pred[0] > 0.5, pred[1] > 0.5, pred[2] > 0.5\n",
    "    pred = (c1 > 0).astype(np.uint8)\n",
    "    pred[(c2 == False) * (c1 == True)] = 2\n",
    "    pred[(c3 == True) * (c1 == True)] = 4\n",
    "\n",
    "    components, n = label(pred == 4)\n",
    "    for et_idx in range(1, n + 1):\n",
    "        _, counts = np.unique(pred[components == et_idx], return_counts=True)\n",
    "        if 1 < counts[0] and counts[0] < 8 and np.mean(enh[components == et_idx]) < 0.9:\n",
    "            pred[components == et_idx] = 1\n",
    "\n",
    "    et = pred == 4\n",
    "    if 0 < et.sum() and et.sum() < 73 and np.mean(enh[et]) < 0.9:\n",
    "        pred[et] = 1\n",
    "\n",
    "    # pred = np.transpose(pred, (2, 1, 0)).astype(np.uint8)\n",
    "    return pred\n",
    "\n",
    "\n",
    "def prepare_preditions(e):\n",
    "    fname = e[0].split(\"/\")[-1].split(\".\")[0][:-6]\n",
    "    print(fname)\n",
    "    preds = [np.load(f) for f in e]\n",
    "    p = to_lbl(np.mean(preds, 0))\n",
    "    return p\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory exists\n",
      "Preparing final predictions\n",
      "Completed!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.makedirs(os.path.join(args.results, \"final_preds\"))\n",
    "except:\n",
    "    print(\"directory exists\")\n",
    "\n",
    "orig_data = 'C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\CC\\\\Backup_2407\\\\val_SSA\\\\monai'\n",
    "                 \n",
    "preds = sorted(glob('/scratch/guest187/Data/val_SSA/results/monai_test/'))\n",
    "examples = list(zip(*[sorted(glob(f\"{p}/*.npy\")) for p in preds]))\n",
    "print(\"Preparing final predictions\")\n",
    "for e in examples:\n",
    "    pred = prepare_preditions(e)\n",
    "    fname = e[0].split(\"/\")[-1].split(\".\")[0][:-6]\n",
    "    outfile = os.path.join(args.results, fname + \".nii.gz\")\n",
    "    img = os.path.join(orig_data, fname, fname + \"-stk.nii.gz\")\n",
    "    save_nifti(np, img, outfile)\n",
    "    \n",
    "print(\"Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise initial predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m max_idx \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfor\u001b[39;00m step, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(val_loader):\n\u001b[0;32m      3\u001b[0m     \u001b[39mif\u001b[39;00m step \u001b[39m<\u001b[39m max_idx:\n\u001b[0;32m      4\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m batch[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'val_loader' is not defined"
     ]
    }
   ],
   "source": [
    "max_idx = 2\n",
    "for step, batch in enumerate(val_loader):\n",
    "    if step < max_idx:\n",
    "        input = batch[0]\n",
    "        print(input.shape)\n",
    "        print(str(batch[1]).split('/')[-1].replace(\"-stk.npy',)\", \"\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_idx = 2\n",
    "with torch.no_grad():\n",
    "    for step, batch in enumerate(val_loader):\n",
    "        if step < max_idx:\n",
    "            input = batch[0]\n",
    "            case = str(batch[1]).split('/')[-1].replace(\"-stk.npy',)\", \"\")\n",
    "            # print(input.shape)\n",
    "            output = inference(VAL_AMP, model, input)\n",
    "            # print(output.shape)\n",
    "            val_output = post_transforms(output[0])\n",
    "            # print(val_output.shape)\n",
    "            \n",
    "            # visualize the 3 channels model output corresponding to this image\n",
    "            plt.figure(\"model output\", (18, 6))\n",
    "            for i in range(3):\n",
    "                plt.subplot(1, 3, i + 1)\n",
    "                plt.title(f\"{case}: output channel {i}\")\n",
    "                plt.imshow(val_output[i, :, :, 75])\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "from glob import glob\n",
    "\n",
    "n, z = 5, 75\n",
    "data = sorted(glob(\"/scratch/guest187/Data/val_SSA/results/monai_test/*.nii.gz\"))\n",
    "for i in range(n):\n",
    "    fname = data[i].split(\"/\")[-1].split(\".\")[0]\n",
    "    print(fname)\n",
    "    img = nib.load(f\"/scratch/guest187/Data/val_SSA/finetune_ssa/images/{fname}-stk.nii.gz\").get_fdata().astype(np.float32)\n",
    "    pred = nib.load(data[i]).get_fdata().astype(np.uint8)[:, :, z]\n",
    "    imgs = [img[:, :, z, i] for i in [0, 3]] + [pred]\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(12, 12))\n",
    "    for i in range(3):\n",
    "        if i < 2:\n",
    "            ax[i].imshow(imgs[i], cmap='gray')\n",
    "        else:\n",
    "            ax[i].imshow(imgs[i]);\n",
    "        ax[i].axis('off')  \n",
    "    plt.tight_layout()            \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\UNN_BraTS23\\\\scripts')\n",
    "import subprocess\n",
    "try:\n",
    "    import os\n",
    "    import numpy as np \n",
    "    import matplotlib.pyplot as plt\n",
    "    import torch\n",
    "    import nibabel as nib\n",
    "    import cv2\n",
    "    from skimage.transform import resize\n",
    "except ModuleNotFoundError as e:\n",
    "    package = str(e).split(\"'\")[0]\n",
    "    subprocess.run(['pip', 'install', package])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_pth = 'C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\CC\\\\Backup_2407\\\\val_SSA\\\\results\\\\monai_test\\\\final_preds'\n",
    "pred_files = [os.path.join(pred_pth, file) for file in os.listdir(pred_pth)]\n",
    "# Calculate the amount of padding needed\n",
    "desired_height = 240\n",
    "desired_width = 240\n",
    "desired_depth = 155\n",
    "\n",
    "\n",
    "for file in pred_files:\n",
    "    pred1 = nib.load(file)\n",
    "    image = pred1.get_fdata().astype(np.uint8)\n",
    "    print(pred.shape)\n",
    "    padding_height = max(0, desired_height - image.shape[0])\n",
    "    padding_width = max(0, desired_width - image.shape[1])\n",
    "    padding_depth = max(0, desired_depth - image.shape[2])\n",
    "  \n",
    "    # Add padding to the image\n",
    "    padded_image = cv2.copyMakeBorder(image, 0, padding_height, 0, padding_width, 0, padding_depth, 0, cv2.BORDER_CONSTANT)\n",
    "\n",
    "    print(padded_image.shape)\n",
    "\n",
    "    # Now 'padded_image' will contain the original image with padding added to achieve the desired dimensions\n",
    "    # Create a new nibabel image with the resized data\n",
    "    resized_image = nib.Nifti1Image(padded_image, pred1.affine, pred1.header)\n",
    "    print(resized_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ======================================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(checkpoint=None):\n",
    "    model=UNet(\n",
    "        spatial_dims=3,\n",
    "        in_channels=4,\n",
    "        out_channels=4,\n",
    "        channels=(16, 32, 64, 128, 256),\n",
    "        # channels=(32, 64, 128, 256, 320, 320), #nnunet channels, depth 6\n",
    "        # channels=(64, 96, 128, 192, 256, 384, 512), # optinet, depth 7\n",
    "        strides=(2, 2, 2, 2), # length should = len(channels) - 1\n",
    "        # kernel_size=,\n",
    "        # num_res_units=,\n",
    "        # dropout=0.0,\n",
    "        )\n",
    "    n_channels = len(model.channels)\n",
    "    print(f\"Number of channels: {n_channels}\")\n",
    "\n",
    "    if checkpoint != None:\n",
    "        model.load_state_dict(torch.load(checkpoint, map_location=torch.device('cpu')))\n",
    "\n",
    "    return model, n_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pth = '/scratch/guest187/Data/train_gli_hack/results_hack_finetune_ssa/checkpoints/f0_finetune/epoch=39-dice=91.38.ckpt'\n",
    "# pth = '/scratch/guest187/Data/train_all/results/test_fullRunThrough/best_metric_model_fullTest.pth'\n",
    "pth = args.ckpt_path\n",
    "checkpoint = torch.load(pth, map_location=torch.device('cpu'))\n",
    "keys = checkpoint.keys()\n",
    "\n",
    "# for key in checkpoint.keys():\n",
    "#     print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "import torchio as tio\n",
    "import logging\n",
    "\n",
    "# import torch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# Import MONAI libraries                <--- CLEAN UP THESE IMPORTS ONCE WE KNOW WHAT libraries are used\n",
    "import monai\n",
    "from monai.config import print_config\n",
    "from monai.data import ArrayDataset, decollate_batch, DataLoader\n",
    "from monai.handlers import (\n",
    "    CheckpointLoader,\n",
    "    IgniteMetric,\n",
    "    MeanDice,\n",
    "    StatsHandler,\n",
    "    TensorBoardImageHandler,\n",
    "    TensorBoardStatsHandler,\n",
    ")\n",
    "from monai.metrics import DiceMetric, LossMetric, HausdorffDistanceMetric\n",
    "from monai.losses import DiceLoss, DiceFocalLoss\n",
    "from monai.networks import nets as monNets\n",
    "from monai.networks.nets import UNet\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    EnsureChannelFirst,\n",
    "    AsDiscrete,\n",
    "    Compose\n",
    ")\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.utils import first\n",
    "from monai.utils.misc import set_determinism\n",
    "\n",
    "# Other imports (unsure)\n",
    "# import ignite\n",
    "import nibabel\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import torch.utils.data as data_utils\n",
    "import json\n",
    "from subprocess import call\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from utils.utils import get_main_args\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from monai.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Given a set of images and corresponding labels (i.e, will give it all training images + labels, and same for val and test)\n",
    "    folder structure: subjectID/subjectID-stk.npy, -lbl.npy (i.e. contains 2 files)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, data_folders, transform=None, SSAtransform=None):\n",
    "            self.data_folders = data_folders                            # path for each data folder in the set\n",
    "            self.transform = transform\n",
    "            self.SSAtransform = SSAtransform\n",
    "            self.imgs = []                                              # store images to load (paths)\n",
    "            self.lbls = []                                              # store corresponding labels (paths)\n",
    "            for img_folder in self.data_folders:                        # run through each subjectID folder\n",
    "                folder_path = os.path.join(data_dir, img_folder)                                                            \n",
    "                self.SSA = True if 'SSA' in img_folder else False       # check if current file is from SSA dataset\n",
    "                for file in os.listdir(folder_path):                    # check folder contents\n",
    "                    if os.path.isfile(os.path.join(folder_path, file)):\n",
    "                        if file.endswith(\"-lbl.npy\"):\n",
    "                            self.lbls.append(os.path.join(folder_path, file))   # Save segmentation mask (file path)\n",
    "                            self.mode = \"labels\"\n",
    "                        elif file.endswith(\"-stk.npy\"):\n",
    "                            self.imgs.append(os.path.join(folder_path, file))   # Save image (file path)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the amount of images in this set\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        name = os.path.dirname(self.imgs[idx])\n",
    "        # Load files\n",
    "        if self.mode == \"labels\":\n",
    "            mask = np.load(self.lbls[idx])\n",
    "            mask = torch.from_numpy(mask) # 240, 240, 155\n",
    "\n",
    "        # print(self.imgs[idx] )\n",
    "        # print(\"========================\")\n",
    "        # print(self.lbls[idx] )\n",
    "        # print(\"========================\")           \n",
    "\n",
    "        if self.transform is not None: # Apply general transformations\n",
    "        # transforms such as crop, flip, rotate etc will be applied to both the image and the mask\n",
    "            if self.mode == \"labels\":\n",
    "                subject = tio.Subject(\n",
    "                    image=tio.ScalarImage(tensor=image),\n",
    "                    mask=tio.LabelMap(tensor=mask)\n",
    "                    )\n",
    "                tranformed_subject = self.transform(subject)\n",
    "                # Apply transformation to GLI data to reduce quality (creating fake SSA data)\n",
    "                if self.SSA == False and self.SSAtransform is not None:\n",
    "                    tranformed_subject = self.SSAtransform(tranformed_subject)\n",
    "            \n",
    "                print(\"Tranformed_subject: \", tranformed_subject)\n",
    "                image = tranformed_subject[\"image\"].data\n",
    "                mask = tranformed_subject[\"mask\"].data\n",
    "                return image, mask, self.imgs[idx]\n",
    "            else:\n",
    "                subject = tio.Subject(\n",
    "                    image=tio.ScalarImage(tensor=image),\n",
    "                    )\n",
    "                tranformed_subject = self.transform(subject)           \n",
    "                print(\"Tranformed_subject: \", tranformed_subject)\n",
    "                image = tranformed_subject[\"image\"].data\n",
    "                return image, self.imgs[idx]\n",
    "\n",
    "        return image, mask, self.imgs[idx]\n",
    "    \n",
    "    def get_paths(self):\n",
    "        return self.img_pth, self.seg_pth\n",
    "    \n",
    "    def get_subj_info(self):\n",
    "        return self.subj_dir_pths, self.subj_dirs\n",
    "        #, self.SSA\n",
    "    \n",
    "    def get_transforms(self):\n",
    "        return self.transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_transforms(n_channels):\n",
    "    # Initialise data transforms\n",
    "    data_transforms = {\n",
    "        'train': tio.Compose([\n",
    "            tio.CropOrPad((192, 192, 124)),\n",
    "            tio.OneOf([\n",
    "                tio.Compose([\n",
    "                    tio.RandomFlip(axes=0, p=0.3),\n",
    "                    tio.RandomFlip(axes=1, p=0.3),\n",
    "                    tio.RandomFlip(axes=2, p=0.3)]),\n",
    "                tio.RandomAffine(degrees=15,p=0.3)\n",
    "            ], p=0.8),\n",
    "            tio.EnsureShapeMultiple(2**n_channels, method='pad')\n",
    "        ]),\n",
    "        'fakeSSA': tio.OneOf({\n",
    "            tio.OneOf({\n",
    "                tio.Compose([\n",
    "                    tio.Resample((1.2, 1.2, 6), scalars_only=True),\n",
    "                    tio.Resample(1)\n",
    "                ]):0.50,\n",
    "                tio.Compose([\n",
    "                    tio.RandomAnisotropy(axes=(1, 2), downsampling=(1.2), scalars_only=True),\n",
    "                    tio.RandomAnisotropy(axes=0, downsampling=(6), scalars_only=True)\n",
    "                ]):0.5,                \n",
    "            },p=0.80),\n",
    "            tio.Compose([            \n",
    "                tio.OneOf({\n",
    "                    tio.RandomBlur(std=(0.5, 1.5)) : 0.3,\n",
    "                    tio.RandomNoise(mean=3, std=(0, 0.33)) : 0.7\n",
    "                },p=0.50),\n",
    "                tio.OneOf({\n",
    "                    tio.RandomMotion(num_transforms=3, image_interpolation='nearest') : 0.5,\n",
    "                    tio.RandomBiasField(coefficients=1) : 0.2,\n",
    "                    tio.RandomGhosting(intensity=1.5) : 0.3\n",
    "                }, p=0.50)\n",
    "            ])\n",
    "        }, p=0.8), # randomly apply ONE of these given transforms with prob 0.5 \n",
    "        'val': tio.Compose([\n",
    "            tio.CropOrPad((192, 192, 124)),\n",
    "            tio.EnsureShapeMultiple(2**n_channels, method='pad')\n",
    "        ]),\n",
    "        'test' : tio.Compose([\n",
    "            tio.EnsureShapeMultiple(2**n_channels, method='pad')\n",
    "        ])\n",
    "    }\n",
    "\n",
    "    return data_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(args, data_transforms):\n",
    "\n",
    "    '''\n",
    "    This function is called during training after define_transforms(n_channels)\n",
    "\n",
    "    It takes as input\n",
    "        args: argparsers from the utils script \n",
    "            args.seed\n",
    "            args.data_used: 'all', 'GLI', 'SSA'\n",
    "        data_transforms: a dictionary of transformations to apply to the data during training\n",
    "\n",
    "    Returns dataloaders ready to be fed into model\n",
    "    '''\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Set a seed for reproducibility if you want the same split - optional\n",
    "    if args.seed != None:\n",
    "        seed=args.seed\n",
    "        logger.info(f\"Seed set to {seed}.\")\n",
    "    else:\n",
    "        seed=None\n",
    "        logger.info(\"No seed has been set\")\n",
    "    \n",
    "    # Locate data based on which dataset is being used\n",
    "    if args.data_used == 'all':\n",
    "        data_folders = glob.glob(os.path.join(args.data, \"BraTS*\"))\n",
    "    elif args.data_used == \"GLI\":\n",
    "        data_folders = [folder for folder in os.listdir(args.data) if 'GLI' in folder]\n",
    "    elif args.data_used == 'SSA':\n",
    "        data_folders = [folder for folder in os.listdir(args.data) if 'SSA' in folder]\n",
    "\n",
    "    # Split data files\n",
    "    train_files, val_files = split_data(data_folders, seed) \n",
    "    logger.info(f\"Number of training files: {len(train_files)}\\nNumber of validation files: {len(val_files)}\")\n",
    "    \n",
    "    image_datasets = {\n",
    "        'train': MRIDataset(args.data, train_files, transform=data_transforms['train']),\n",
    "        'val': MRIDataset(args.data, val_files, transform=data_transforms['val']),\n",
    "    }\n",
    "\n",
    "    # Create dataloaders\n",
    "    # can set num_workers for running sub-processes\n",
    "    dataloaders = {\n",
    "        'train': data_utils.DataLoader(image_datasets['train'], batch_size=args.batch_size, shuffle=True, drop_last=True),\n",
    "        'val': data_utils.DataLoader(image_datasets['val'], batch_size=args.val_batch_size, shuffle=True)\n",
    "        # 'test': data_utils.DataLoader(image_datasets['test'], batch_size=args.val_batch_size, shuffle=True)\n",
    "    }\n",
    "\n",
    "    # Save data split\n",
    "    splitData = {\n",
    "        'subjsTr' : train_files,\n",
    "        'subjsVal' : val_files,\n",
    "        # 'subjsTest' : test_files    \n",
    "    }\n",
    "    with open(args.data + str(args.data_used) + \".json\", \"w\") as file:\n",
    "        json.dump(splitData, file)\n",
    "\n",
    "    return dataloaders\n",
    "\n",
    "def split_data(data_folders, seed):\n",
    "    '''\n",
    "    Function to split dataset into train/val/test splits, given all avilable data.\n",
    "    Input:\n",
    "        list of paths to numpy files\n",
    "    Returns:\n",
    "        lists for each train and val/test sets, where each list contains the file names to be used in the set\n",
    "    '''\n",
    "    #-----------------------------\n",
    "    # originally we split as 3: train-test-val train (70), val (15), test (15):\n",
    "        # train_files, test_files = train_test_split(data_folders, test_size=0.7, random_state=seed)\n",
    "        # val_files, test_files = train_test_split(test_files, test_size=0.5, random_state=seed)\n",
    "\n",
    "    #-----------------------------\n",
    "    # training loop split is train-val (70-30)\n",
    "    train_files, val_files = train_test_split(data_folders, test_size=0.7, random_state=seed)\n",
    "\n",
    "    # ??? validation/testing???\n",
    "\n",
    "    return train_files, val_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Setup transforms, dataset\"\"\"\n",
    "def define_dataloaders(n_channels):\n",
    "    # Define transforms\n",
    "    data_transform = define_transforms(n_channels)\n",
    "    # Load data\n",
    "    dataloaders = load_data(args, data_transform)                      # this also saves a json splitData\n",
    "    # train_loader, val_loader = dataloaders['train'], dataloaders['val']\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define model architecture:\n",
    "        Done before data loader so that transforms has n_channels for EnsureShapeMultiple\n",
    "\"\"\"\n",
    "def define_model(checkpoint=None):\n",
    "    model=UNet(\n",
    "        spatial_dims=3,\n",
    "        in_channels=4,\n",
    "        out_channels=4,\n",
    "        channels=(16, 32, 64, 128, 256),\n",
    "        # channels=(32, 64, 128, 256, 320, 320), #nnunet channels, depth 6\n",
    "        # channels=(64, 96, 128, 192, 256, 384, 512), # optinet, depth 7\n",
    "        strides=(2, 2, 2, 2), # length should = len(channels) - 1\n",
    "        # kernel_size=,\n",
    "        # num_res_units=,\n",
    "        # dropout=0.0,\n",
    "        )\n",
    "    n_channels = len(model.channels)\n",
    "    print(f\"Number of channels: {n_channels}\")\n",
    "\n",
    "    if checkpoint != None:\n",
    "        model.load_state_dict(torch.load(checkpoint, map_location=torch.device('cpu')))\n",
    "\n",
    "    return model, n_channels\n",
    "\n",
    "\"\"\"\n",
    "Setup validation stuff\n",
    "    metrics\n",
    "    post trans ???????\n",
    "    define inference\n",
    "\"\"\"\n",
    "def val_params():\n",
    "    VAL_AMP = True\n",
    "    dice_metric = DiceMetric(include_background=True, reduction=\"mean\", get_not_nans=True, num_classes=4)\n",
    "    dice_metric_batch = DiceMetric(include_background=True, reduction=\"mean_batch\", get_not_nans=True, num_classes=4)\n",
    "    post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
    "    return VAL_AMP, dice_metric, dice_metric_batch, post_trans\n",
    "\n",
    "# define inference method\n",
    "def inference(VAL_AMP, model, input):\n",
    "    def _compute(input):\n",
    "        return sliding_window_inference(\n",
    "            inputs=input,\n",
    "            roi_size=None, ## best to leave as None, computes itself (else NEEDS TO BE BASED ON LAYER IN, OUT E.G., Conv3d(4, 16; Conv3d(16, 32; etc)\n",
    "            # roi_size=(128, 128, 64),\n",
    "            sw_batch_size=1,\n",
    "            predictor=model,\n",
    "            overlap=0.5,\n",
    "            mode='gaussian'\n",
    "        )\n",
    "    if VAL_AMP:\n",
    "        with torch.cuda.amp.autocast():\n",
    "            return _compute(input)\n",
    "    else:\n",
    "        return _compute(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"General Setup: \n",
    "    logging,\n",
    "    utils.args \n",
    "    seed,\n",
    "    cuda, \n",
    "    root dir\"\"\"\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "# args = get_main_args()\n",
    "seed = 42\n",
    "set_determinism(seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# root_dir = args.data\n",
    "# results_dir = args.results\n",
    "\n",
    "model, n_channels = define_model()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File paths for Alex's local system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dir= '/Users/alexandrasmith/Desktop/Workspace/Projects/UNN_BraTS23/data/val_SSA'\n",
    "validation_files = [os.path.join(validation_dir, file) for file in os.listdir(validation_dir)]\n",
    "\n",
    "# checkpoint to test: /scratch/guest187/Data/train_all/results/test_run/best_metric_model.pth \n",
    "checkpoint = '/Users/alexandrasmith/Desktop/Workspace/Projects/UNN_BraTS23/data/best_metric_model_fullTest.pth'\n",
    "\n",
    "# Load validation data to dataloader\n",
    "data_transforms = define_transforms(n_channels)\n",
    "validation_dataset = MRIDataset(validation_dir, validation_files, transform=data_transforms['val'])\n",
    "\n",
    "# Validation parameters\n",
    "VAL_AMP, dice_metric, dice_metric_batch, post_transforms = val_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check best model output with the input image and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(validation_dataset))\n",
    "img, image_path = validation_dataset[0]\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, n_channels = define_model(checkpoint)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # select one image to evaluate and visualize the model output\n",
    "    val_input, _ = validation_dataset[0]\n",
    "    print(val_input.shape)\n",
    "    # roi_size = (128, 128, 64)\n",
    "    # sw_batch_size = 4\n",
    "    val_output = inference(VAL_AMP, model, val_input)\n",
    "    val_output = post_transforms(val_output[0])\n",
    "    plt.figure(\"image\", (24, 6))\n",
    "    for i in range(4):\n",
    "        plt.subplot(1, 4, i + 1)\n",
    "        plt.title(f\"image channel {i}\")\n",
    "        plt.imshow(validation_dataset[6][\"image\"][i, :, :, 70].detach().cpu(), cmap=\"gray\")\n",
    "    plt.show()\n",
    "    # visualize the 3 channels label corresponding to this image\n",
    "    plt.figure(\"label\", (18, 6))\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        plt.title(f\"label channel {i}\")\n",
    "        plt.imshow(validation_dataset[6][\"label\"][i, :, :, 70].detach().cpu())\n",
    "    plt.show()\n",
    "    # visualize the 3 channels model output corresponding to this image\n",
    "    plt.figure(\"output\", (18, 6))\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        plt.title(f\"output channel {i}\")\n",
    "        plt.imshow(val_output[i, :, :, 70].detach().cpu())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

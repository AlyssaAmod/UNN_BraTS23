{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67c9b15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "import torchio as tio\n",
    "import logging\n",
    "\n",
    "# import torch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# Import MONAI libraries                <--- CLEAN UP THESE IMPORTS ONCE WE KNOW WHAT libraries are used\n",
    "import monai\n",
    "from monai.config import print_config\n",
    "from monai.data import ArrayDataset, decollate_batch, DataLoader\n",
    "from monai.handlers import (\n",
    "    CheckpointLoader,\n",
    "    IgniteMetric,\n",
    "    MeanDice,\n",
    "    StatsHandler,\n",
    "    TensorBoardImageHandler,\n",
    "    TensorBoardStatsHandler,\n",
    ")\n",
    "from monai.metrics import DiceMetric, LossMetric, HausdorffDistanceMetric\n",
    "from monai.losses import DiceLoss, DiceFocalLoss\n",
    "from monai.networks import nets as monNets\n",
    "from monai.networks.nets import UNet\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    EnsureChannelFirst,\n",
    "    AsDiscrete,\n",
    "    Compose\n",
    ")\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.utils import first\n",
    "from monai.utils.misc import set_determinism\n",
    "\n",
    "# Other imports (unsure)\n",
    "# import ignite\n",
    "import nibabel\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import torch.utils.data as data_utils\n",
    "import json\n",
    "from subprocess import call\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from utils.utils import get_main_args\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from monai.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306da614",
   "metadata": {},
   "source": [
    "# Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "20d60944",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Given a set of images and corresponding labels (i.e, will give it all training images + labels, and same for val and test)\n",
    "    folder structure: subjectID/subjectID-stk.npy, -lbl.npy (i.e. contains 2 files)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, data_folders, transform=None, SSAtransform=None):\n",
    "            self.data_folders = data_folders                            # path for each data folder in the set\n",
    "            self.transform = transform\n",
    "            self.SSAtransform = SSAtransform\n",
    "            self.mode = 'val'\n",
    "            self.imgs = []                                              # store images to load (paths)\n",
    "            self.lbls = []                                              # store corresponding labels (paths)\n",
    "            for img_folder in self.data_folders:                        # run through each subjectID folder\n",
    "                folder_path = os.path.join(data_dir, img_folder)                                                            \n",
    "                self.SSA = True if 'SSA' in img_folder else False       # check if current file is from SSA dataset\n",
    "                for file in os.listdir(folder_path):                    # check folder contents\n",
    "                    if os.path.isfile(os.path.join(folder_path, file)):\n",
    "                        if file.endswith(\"-lbl.npy\"):\n",
    "                            self.lbls.append(os.path.join(folder_path, file))   # Save segmentation mask (file path)\n",
    "                        elif file.endswith(\"-stk.npy\"):\n",
    "                            self.imgs.append(os.path.join(folder_path, file))   # Save image (file path)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the amount of images in this set\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        name = os.path.dirname(self.imgs[idx])\n",
    "        # Load files\n",
    "        image = np.load(self.imgs[idx])\n",
    "        image = torch.from_numpy(image) # 4, 240, 240, 155\n",
    "        if self.mode == 'train':\n",
    "            mask = np.load(self.lbls[idx])\n",
    "            mask = torch.from_numpy(mask) # 240, 240, 155\n",
    "\n",
    "        # print(self.imgs[idx] )\n",
    "        # print(\"========================\")\n",
    "        # print(self.lbls[idx] )\n",
    "        # print(\"========================\")           \n",
    "\n",
    "        if self.transform is not None: # Apply general transformations\n",
    "        # transforms such as crop, flip, rotate etc will be applied to both the image and the mask\n",
    "            if self.mode == 'train':\n",
    "                subject = tio.Subject(\n",
    "                    image=tio.ScalarImage(tensor=image),\n",
    "                    mask=tio.LabelMap(tensor=mask)\n",
    "                    )\n",
    "                tranformed_subject = self.transform(subject)\n",
    "                # Apply transformation to GLI data to reduce quality (creating fake SSA data)\n",
    "                if self.SSA == False and self.SSAtransform is not None:\n",
    "                    tranformed_subject = self.SSAtransform(tranformed_subject)\n",
    "            \n",
    "                print(\"Tranformed_subject: \", tranformed_subject)\n",
    "                image = tranformed_subject[\"image\"].data\n",
    "                mask = tranformed_subject[\"mask\"].data\n",
    "                return image, mask, self.imgs[idx]\n",
    "            else:\n",
    "                subject = tio.Subject(\n",
    "                    image=tio.ScalarImage(tensor=image),\n",
    "                    )\n",
    "                tranformed_subject = self.transform(subject)           \n",
    "                print(\"Tranformed_subject: \", tranformed_subject)\n",
    "                image = tranformed_subject[\"image\"].data\n",
    "                return image, self.imgs[idx]\n",
    "\n",
    "        return image, mask, self.imgs[idx]\n",
    "    \n",
    "    def get_paths(self):\n",
    "        return self.img_pth, self.seg_pth\n",
    "    \n",
    "    def get_subj_info(self):\n",
    "        return self.subj_dir_pths, self.subj_dirs\n",
    "        #, self.SSA\n",
    "    \n",
    "    def get_transforms(self):\n",
    "        return self.transform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84183f85",
   "metadata": {},
   "source": [
    "# Data Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "df2df819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_transforms(n_channels):\n",
    "    # Initialise data transforms\n",
    "    data_transforms = {\n",
    "        'train': tio.Compose([\n",
    "            tio.CropOrPad((192, 192, 124)),\n",
    "            tio.OneOf([\n",
    "                tio.Compose([\n",
    "                    tio.RandomFlip(axes=0, p=0.3),\n",
    "                    tio.RandomFlip(axes=1, p=0.3),\n",
    "                    tio.RandomFlip(axes=2, p=0.3)]),\n",
    "                tio.RandomAffine(degrees=15,p=0.3)\n",
    "            ], p=0.8),\n",
    "            tio.EnsureShapeMultiple(2**n_channels, method='pad')\n",
    "        ]),\n",
    "        'fakeSSA': tio.OneOf({\n",
    "            tio.OneOf({\n",
    "                tio.Compose([\n",
    "                    tio.Resample((1.2, 1.2, 6), scalars_only=True),\n",
    "                    tio.Resample(1)\n",
    "                ]):0.50,\n",
    "                tio.Compose([\n",
    "                    tio.RandomAnisotropy(axes=(1, 2), downsampling=(1.2), scalars_only=True),\n",
    "                    tio.RandomAnisotropy(axes=0, downsampling=(6), scalars_only=True)\n",
    "                ]):0.5,                \n",
    "            },p=0.80),\n",
    "            tio.Compose([            \n",
    "                tio.OneOf({\n",
    "                    tio.RandomBlur(std=(0.5, 1.5)) : 0.3,\n",
    "                    tio.RandomNoise(mean=3, std=(0, 0.33)) : 0.7\n",
    "                },p=0.50),\n",
    "                tio.OneOf({\n",
    "                    tio.RandomMotion(num_transforms=3, image_interpolation='nearest') : 0.5,\n",
    "                    tio.RandomBiasField(coefficients=1) : 0.2,\n",
    "                    tio.RandomGhosting(intensity=1.5) : 0.3\n",
    "                }, p=0.50)\n",
    "            ])\n",
    "        }, p=0.8), # randomly apply ONE of these given transforms with prob 0.5 \n",
    "        'val': tio.Compose([\n",
    "            tio.CropOrPad((192, 192, 124)),\n",
    "            tio.EnsureShapeMultiple(2**n_channels, method='pad')\n",
    "        ]),\n",
    "        'test' : tio.Compose([\n",
    "            tio.EnsureShapeMultiple(2**n_channels, method='pad')\n",
    "        ])\n",
    "    }\n",
    "\n",
    "    return data_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d6d13677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.0.conv.weight\n",
      "model.0.conv.bias\n",
      "model.0.adn.A.weight\n",
      "model.1.submodule.0.conv.weight\n",
      "model.1.submodule.0.conv.bias\n",
      "model.1.submodule.0.adn.A.weight\n",
      "model.1.submodule.1.submodule.0.conv.weight\n",
      "model.1.submodule.1.submodule.0.conv.bias\n",
      "model.1.submodule.1.submodule.0.adn.A.weight\n",
      "model.1.submodule.1.submodule.1.submodule.0.conv.weight\n",
      "model.1.submodule.1.submodule.1.submodule.0.conv.bias\n",
      "model.1.submodule.1.submodule.1.submodule.0.adn.A.weight\n",
      "model.1.submodule.1.submodule.1.submodule.1.submodule.conv.weight\n",
      "model.1.submodule.1.submodule.1.submodule.1.submodule.conv.bias\n",
      "model.1.submodule.1.submodule.1.submodule.1.submodule.adn.A.weight\n",
      "model.1.submodule.1.submodule.1.submodule.2.conv.weight\n",
      "model.1.submodule.1.submodule.1.submodule.2.conv.bias\n",
      "model.1.submodule.1.submodule.1.submodule.2.adn.A.weight\n",
      "model.1.submodule.1.submodule.2.conv.weight\n",
      "model.1.submodule.1.submodule.2.conv.bias\n",
      "model.1.submodule.1.submodule.2.adn.A.weight\n",
      "model.1.submodule.2.conv.weight\n",
      "model.1.submodule.2.conv.bias\n",
      "model.1.submodule.2.adn.A.weight\n",
      "model.2.conv.weight\n",
      "model.2.conv.bias\n"
     ]
    }
   ],
   "source": [
    "# pth = '/scratch/guest187/Data/train_gli_hack/results_hack_finetune_ssa/checkpoints/f0_finetune/epoch=39-dice=91.38.ckpt'\n",
    "pth = '/scratch/guest187/Data/train_all/results/test_fullRunThrough/best_metric_model_fullTest.pth'\n",
    "checkpoint = torch.load(pth, map_location=torch.device('cpu'))\n",
    "keys = checkpoint.keys()\n",
    "\n",
    "for key in checkpoint.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c4f9f7bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate_dict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'state_dict'"
     ]
    }
   ],
   "source": [
    "print(checkpoint[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "58fa3dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'patch_size': [128, 128, 128], 'spacings': [1.0, 1.0, 1.0], 'n_class': 4, 'in_channels': 5}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "cnfgP = '/scratch/guest187/Data/train_all_nnUNET/train_all_data/results/11_3d/config.pkl'\n",
    "config = pickle.load(open(cnfgP, \"rb\"))\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "004dae04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]]\n",
      "[[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]\n"
     ]
    }
   ],
   "source": [
    "patch_size, spacings = config[\"patch_size\"], config[\"spacings\"]\n",
    "strides, kernels, sizes = [], [], patch_size[:]\n",
    "while True:\n",
    "    spacing_ratio = [spacing / min(spacings) for spacing in spacings]\n",
    "    stride = [\n",
    "        2 if ratio <= 2 and size >= 2 * 2 else 1 for (ratio, size) in zip(spacing_ratio, sizes)\n",
    "    ]\n",
    "    kernel = [3 if ratio <= 2 else 1 for ratio in spacing_ratio]\n",
    "    if all(s == 1 for s in stride):\n",
    "        break\n",
    "    sizes = [i / j for i, j in zip(sizes, stride)]\n",
    "    spacings = [i * j for i, j in zip(spacings, stride)]\n",
    "    kernels.append(kernel)\n",
    "    strides.append(stride)\n",
    "    if len(strides) == 6:\n",
    "        break\n",
    "strides.insert(0, len(spacings) * [1])\n",
    "kernels.append(len(spacings) * [3])\n",
    "\n",
    "print(strides)\n",
    "print(kernels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8bbe44",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7652c8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "set_determinism(seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def define_model(checkpoint=None):\n",
    "    \"\"\"Define model architecture:\n",
    "        Done before data loader so that transforms has n_channels for EnsureShapeMultiple\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    model=UNet(\n",
    "        spatial_dims=3,\n",
    "        in_channels=4,\n",
    "        out_channels=4,\n",
    "        channels=(16, 32, 64, 128, 256),\n",
    "        # channels=(32, 64, 128, 256, 320, 320), #nnunet channels, depth 6\n",
    "        # channels=(64, 96, 128, 192, 256, 384, 512), # optinet, depth 7\n",
    "        strides=(2, 2, 2, 2), # length should = len(channels) - 1\n",
    "        # kernel_size=,\n",
    "        # num_res_units=,\n",
    "        # dropout=0.0,\n",
    "        )\n",
    "#     model= monNets.DynUNet(\n",
    "#                 spatial_dims=3,\n",
    "#                 in_channels=5,\n",
    "#                 out_channels=4,\n",
    "#                 kernel_size=[[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]],\n",
    "#                 strides=[[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]],\n",
    "#                 upsample_kernel_size=[[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]],\n",
    "#                 filters=[64, 96, 128, 192, 256, 384, 512],\n",
    "#                 norm_name=(\"instance\", {\"affine\": True}),\n",
    "#                 act_name=(\"leakyrelu\", {\"inplace\": False, \"negative_slope\": 0.01}),\n",
    "#                 deep_supervision=True,\n",
    "#                 res_block=False,\n",
    "#                 trans_bias=True)\n",
    "    \n",
    "    model.to(device)\n",
    "    n_channels = len(model.channels)\n",
    "    logger.info(f\"Number of channels: {n_channels}\")\n",
    "\n",
    "    if checkpoint != None:\n",
    "        ckpt = torch.load(checkpoint, map_location=torch.device('cpu'))\n",
    "#         state_dict = (ckpt[\"state_dict\"])    \n",
    "        model.load_state_dict(ckpt)\n",
    "\n",
    "    return model, n_channels\n",
    "\n",
    "\"\"\"\n",
    "Setup transforms, dataset\n",
    "\"\"\"\n",
    "def define_dataloaders(n_channels):\n",
    "    data_transform = define_transforms(n_channels)       # Define transforms\n",
    "    dataloaders = load_data(args, data_transform)        # Load data; also saves json splitData\n",
    "    # train_loader, val_loader = dataloaders['train'], dataloaders['val']\n",
    "    return dataloaders\n",
    "\n",
    "\"\"\"\n",
    "Setup validation stuff\n",
    "    metrics\n",
    "    post trans ???????\n",
    "    define inference\n",
    "\"\"\"\n",
    "def val_params():\n",
    "    VAL_AMP = True\n",
    "    dice_metric = DiceMetric(include_background=True, reduction=\"mean\", get_not_nans=True, num_classes=4)\n",
    "    dice_metric_batch = DiceMetric(include_background=True, reduction=\"mean_batch\", get_not_nans=True, num_classes=4)\n",
    "    post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
    "    return VAL_AMP, dice_metric, dice_metric_batch, post_trans\n",
    "\n",
    "# define inference method\n",
    "def inference(VAL_AMP, model, input):\n",
    "    def _compute(input):\n",
    "        '''\n",
    "        roi_size – the spatial window size for inferences. \n",
    "        When its components have None or non-positives, the corresponding inputs dimension will be used. \n",
    "        if the components of the roi_size are non-positive values, the transform will use the corresponding components of img size.\n",
    "        For example, roi_size=(32, -1) will be adapted to (32, 64) if the second spatial dimension size of img is 64\n",
    "        '''\n",
    "        return sliding_window_inference(\n",
    "            inputs=input,\n",
    "            roi_size=None,\n",
    "            sw_batch_size=4, #sw_batch_size denotes the max number of windows per network inference iteration, not the batch size of inputs.\n",
    "            predictor=model,\n",
    "            overlap=0.2,\n",
    "            mode='constant'\n",
    "        )\n",
    "    if VAL_AMP:\n",
    "        with torch.cuda.amp.autocast():\n",
    "            return _compute(input)\n",
    "    else:\n",
    "        return _compute(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2c8975d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/scratch/guest187/Data/val_SSA/BraTS-SSA-00139-000', '/scratch/guest187/Data/val_SSA/BraTS-SSA-00227-000', '/scratch/guest187/Data/val_SSA/BraTS-SSA-00126-000', '/scratch/guest187/Data/val_SSA/BraTS-SSA-00192-000', '/scratch/guest187/Data/val_SSA/BraTS-SSA-00143-000', '/scratch/guest187/Data/val_SSA/BraTS-SSA-00169-000', '/scratch/guest187/Data/val_SSA/BraTS-SSA-00129-000', '/scratch/guest187/Data/val_SSA/BraTS-SSA-00180-000', '/scratch/guest187/Data/val_SSA/BraTS-SSA-00158-000', '/scratch/guest187/Data/val_SSA/BraTS-SSA-00132-000', '/scratch/guest187/Data/val_SSA/BraTS-SSA-00198-000', '/scratch/guest187/Data/val_SSA/BraTS-SSA-00210-000', '/scratch/guest187/Data/val_SSA/BraTS-SSA-00188-000', '/scratch/guest187/Data/val_SSA/BraTS-SSA-00218-000', '/scratch/guest187/Data/val_SSA/BraTS-SSA-00148-000']\n",
      "15\n",
      "Tranformed_subject:  Subject(Keys: ('image',); images: 1)\n",
      "torch.Size([4, 192, 192, 128])\n"
     ]
    }
   ],
   "source": [
    "validation_dir='/scratch/guest187/Data/val_SSA'\n",
    "validation_files = [os.path.join(validation_dir, file) for file in os.listdir(validation_dir) if not (file.endswith(\".json\") or file == 'images' or file == 'labels' or file == 'ATr_prepoc')]\n",
    "\n",
    "print(validation_files)\n",
    "# checkpoint to test: finetuned hackathon model brats21\n",
    "# checkpoint = '/scratch/guest187/Data/train_gli_hack/results_hack_finetune_ssa/checkpoints/f0_finetune/epoch=39-dice=91.38.ckpt'\n",
    "checkpoint2 = '/scratch/guest187/Data/train_all/results/test_fullRunThrough/best_metric_model_fullTest.pth'\n",
    "\n",
    "model, n_channels = define_model(checkpoint2)\n",
    "\n",
    "\n",
    "# Load validation data to dataloader\n",
    "data_transforms = define_transforms(n_channels)\n",
    "validation_dataset = MRIDataset(validation_dir, validation_files, transform=data_transforms['val'])\n",
    "\n",
    "print(len(validation_dataset))\n",
    "img, image_path = validation_dataset[0]\n",
    "print(img.shape)\n",
    "\n",
    "# Validation parameters\n",
    "VAL_AMP, dice_metric, dice_metric_batch, post_transforms = val_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4a5309d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tranformed_subject:  Subject(Keys: ('image',); images: 1)\n",
      "torch.Size([4, 192, 192, 128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 128 but got size 256 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[133], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m val_input, _ \u001b[38;5;241m=\u001b[39m validation_dataset[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(val_input\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 11\u001b[0m val_output \u001b[38;5;241m=\u001b[39m \u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mVAL_AMP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m val_output \u001b[38;5;241m=\u001b[39m post_transforms(val_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;241m24\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "Cell \u001b[0;32mIn[131], line 89\u001b[0m, in \u001b[0;36minference\u001b[0;34m(VAL_AMP, model, input)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m VAL_AMP:\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[0;32m---> 89\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _compute(\u001b[38;5;28minput\u001b[39m)\n",
      "Cell \u001b[0;32mIn[131], line 79\u001b[0m, in \u001b[0;36minference.<locals>._compute\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_compute\u001b[39m(\u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m     73\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m    roi_size – the spatial window size for inferences. \u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m    When its components have None or non-positives, the corresponding inputs dimension will be used. \u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03m    if the components of the roi_size are non-positive values, the transform will use the corresponding components of img size.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m    For example, roi_size=(32, -1) will be adapted to (32, 64) if the second spatial dimension size of img is 64\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msliding_window_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroi_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43msw_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#sw_batch_size denotes the max number of windows per network inference iteration, not the batch size of inputs.\u001b[39;49;00m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconstant\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/hackathon/lib/python3.9/site-packages/monai/inferers/utils.py:223\u001b[0m, in \u001b[0;36msliding_window_inference\u001b[0;34m(inputs, roi_size, sw_batch_size, predictor, overlap, mode, sigma_scale, padding_mode, cval, sw_device, device, progress, roi_weight_map, process_fn, buffer_steps, buffer_dim, *args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m     win_data \u001b[38;5;241m=\u001b[39m inputs[unravel_slice[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39mto(sw_device)\n\u001b[0;32m--> 223\u001b[0m seg_prob_out \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwin_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# batched patch\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# convert seg_prob_out to tuple seg_tuple, this does not allocate new memory.\u001b[39;00m\n\u001b[1;32m    226\u001b[0m dict_keys, seg_tuple \u001b[38;5;241m=\u001b[39m _flatten_struct(seg_prob_out)\n",
      "File \u001b[0;32m~/hackathon/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/hackathon/lib/python3.9/site-packages/monai/networks/nets/unet.py:303\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 303\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/hackathon/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/hackathon/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/hackathon/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/hackathon/lib/python3.9/site-packages/monai/networks/layers/simplelayers.py:129\u001b[0m, in \u001b[0;36mSkipConnection.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 129\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat([x, y], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim)\n",
      "File \u001b[0;32m~/hackathon/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/hackathon/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/hackathon/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/hackathon/lib/python3.9/site-packages/monai/networks/layers/simplelayers.py:129\u001b[0m, in \u001b[0;36mSkipConnection.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 129\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat([x, y], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim)\n",
      "File \u001b[0;32m~/hackathon/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/hackathon/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/hackathon/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/hackathon/lib/python3.9/site-packages/monai/networks/layers/simplelayers.py:129\u001b[0m, in \u001b[0;36mSkipConnection.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 129\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat([x, y], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim)\n",
      "File \u001b[0;32m~/hackathon/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/hackathon/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/hackathon/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/hackathon/lib/python3.9/site-packages/monai/networks/layers/simplelayers.py:132\u001b[0m, in \u001b[0;36mSkipConnection.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    129\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmodule(x)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39madd(x, y)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 128 but got size 256 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "model, n_channels = define_model(checkpoint2)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # select one image to evaluate and visualize the model output\n",
    "    val_input, _ = validation_dataset[0]\n",
    "    print(val_input.shape)\n",
    "    val_output = inference(VAL_AMP, model, val_input)\n",
    "    val_output = post_transforms(val_output[0])\n",
    "    plt.figure(\"image\", (24, 6))\n",
    "    for i in range(4):\n",
    "        plt.subplot(1, 4, i + 1)\n",
    "        plt.title(f\"image channel {i}\")\n",
    "        plt.imshow(validation_dataset[6][\"image\"][i, :, :, 70].detach().cpu(), cmap=\"gray\")\n",
    "    plt.show()\n",
    "    # visualize the 3 channels label corresponding to this image\n",
    "    plt.figure(\"label\", (18, 6))\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        plt.title(f\"label channel {i}\")\n",
    "        plt.imshow(validation_dataset[6][\"label\"][i, :, :, 70].detach().cpu())\n",
    "    plt.show()\n",
    "    # visualize the 3 channels model output corresponding to this image\n",
    "    plt.figure(\"output\", (18, 6))\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        plt.title(f\"output channel {i}\")\n",
    "        plt.imshow(val_output[i, :, :, 70].detach().cpu())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34703946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e5642b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

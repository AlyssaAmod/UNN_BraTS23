{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandrasmith/miniforge3/envs/spark/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "import torchio as tio\n",
    "import logging\n",
    "\n",
    "# import torch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# Import MONAI libraries                <--- CLEAN UP THESE IMPORTS ONCE WE KNOW WHAT libraries are used\n",
    "import monai\n",
    "from monai.config import print_config\n",
    "from monai.data import ArrayDataset, decollate_batch, DataLoader\n",
    "from monai.handlers import (\n",
    "    CheckpointLoader,\n",
    "    IgniteMetric,\n",
    "    MeanDice,\n",
    "    StatsHandler,\n",
    "    TensorBoardImageHandler,\n",
    "    TensorBoardStatsHandler,\n",
    ")\n",
    "from monai.metrics import DiceMetric, LossMetric, HausdorffDistanceMetric\n",
    "from monai.losses import DiceLoss, DiceFocalLoss\n",
    "from monai.networks import nets as monNets\n",
    "from monai.networks.nets import UNet\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    EnsureChannelFirst,\n",
    "    AsDiscrete,\n",
    "    Compose\n",
    ")\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.utils import first\n",
    "from monai.utils.misc import set_determinism\n",
    "\n",
    "# Other imports (unsure)\n",
    "# import ignite\n",
    "import nibabel\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import torch.utils.data as data_utils\n",
    "import json\n",
    "from subprocess import call\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from utils.utils import get_main_args\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from monai.data import Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Given a set of images and corresponding labels (i.e will give it all training images + labels, and same for val and test)\n",
    "    folder structure: subjectID/image.nii, seg.nii (i.e. contains 2 files)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, data_folders, transform=None, SSAtransform=None):\n",
    "            self.data_folders = data_folders # path for each data folder in the set\n",
    "            self.transform = transform\n",
    "            self.SSAtransform = SSAtransform\n",
    "            self.imgs = [] # store images to load (paths)\n",
    "            self.lbls = [] # store corresponding labels (paths)\n",
    "            # run through each subjectID folder\n",
    "            for img_folder in self.data_folders:\n",
    "                folder_path = os.path.join(data_dir, img_folder)\n",
    "                # check if current file is from SSA dataset\n",
    "                self.SSA = True if 'SSA' in img_folder else False\n",
    "                for file in os.listdir(folder_path):\n",
    "                    # check folder contents\n",
    "                    if os.path.isfile(os.path.join(folder_path, file)):\n",
    "                        # Save segmentation mask (file path)\n",
    "                        if file.endswith(\"-lbl.npy\"):\n",
    "                            self.lbls.append(os.path.join(folder_path, file))\n",
    "                        elif file.endswith(\"-stk.npy\"):\n",
    "                            # Save image (file path)\n",
    "                            self.imgs.append(os.path.join(folder_path, file))\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the amount of images in this set\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Load files\n",
    "        image = np.load(self.imgs[idx])\n",
    "        # mask = np.load(self.lbls[idx])\n",
    "        # print(self.imgs[idx] )\n",
    "        # print(\"========================\")\n",
    "        # print(self.lbls[idx] )\n",
    "        # print(\"========================\")        \n",
    "        # Convert to tensor\n",
    "        image = torch.from_numpy(image) # 4, 240, 240, 155\n",
    "        # mask = torch.from_numpy(mask) # 240, 240, 155\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        if self.transform is not None: # Apply general transformations\n",
    "            # transforms such as crop, flip, rotate etc will be applied to both the image and the mask\n",
    "            # image = self.transform(image.to(device))\n",
    "            # mask = self.transform(mask.to(device))\n",
    "            subject = tio.Subject(\n",
    "                image=tio.ScalarImage(tensor=image),\n",
    "                # mask=tio.LabelMap(tensor=mask)\n",
    "                )\n",
    "            tranformed_subject = self.transform(subject) \n",
    "            print(\"tranformed_subject: \", tranformed_subject)\n",
    "            image = tranformed_subject[\"image\"].data\n",
    "            # mask = tranformed_subject[\"mask\"].data\n",
    "        if self.SSA == False and self.SSAtransform is not None: # Apply transformation to GLI data to reduce quality (creating fake SSA data)\n",
    "            # transforms such as blur, noise etc are NOT applied to mask as well\n",
    "            # image = self.SSAtransform(image.to(device))\n",
    "            image = self.SSAtransform(image)\n",
    "        \n",
    "        return image, self.imgs[idx]\n",
    "    \n",
    "    def get_paths(self):\n",
    "        return self.img_pth, self.seg_pth\n",
    "    \n",
    "    def get_subj_info(self):\n",
    "        return self.subj_dir_pths, self.subj_dirs\n",
    "        #, self.SSA\n",
    "    \n",
    "    def get_transforms(self):\n",
    "        return self.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_transforms(n_channels):\n",
    "    # Initialise data transforms\n",
    "    data_transforms = {\n",
    "        'train': tio.Compose([\n",
    "            tio.CropOrPad((192, 192, 124)),\n",
    "            tio.OneOf([\n",
    "                tio.Compose([\n",
    "                    tio.RandomFlip(axes=0, p=0.3),\n",
    "                    tio.RandomFlip(axes=1, p=0.3),\n",
    "                    tio.RandomFlip(axes=2, p=0.3)]),\n",
    "                tio.RandomAffine(degrees=15,p=0.3)\n",
    "            ], p=0.8),\n",
    "            tio.EnsureShapeMultiple(2**n_channels, method='pad')\n",
    "        ]),\n",
    "        'fakeSSA': tio.OneOf({\n",
    "            tio.OneOf({\n",
    "                tio.Compose([\n",
    "                    tio.Resample((1.2, 1.2, 6), scalars_only=True),\n",
    "                    tio.Resample(1)\n",
    "                ]):0.50,\n",
    "                tio.Compose([\n",
    "                    tio.RandomAnisotropy(axes=(1, 2), downsampling=(1.2), scalars_only=True),\n",
    "                    tio.RandomAnisotropy(axes=0, downsampling=(6), scalars_only=True)\n",
    "                ]):0.5,                \n",
    "            },p=0.80),\n",
    "            tio.Compose([            \n",
    "                tio.OneOf({\n",
    "                    tio.RandomBlur(std=(0.5, 1.5)) : 0.3,\n",
    "                    tio.RandomNoise(mean=3, std=(0, 0.33)) : 0.7\n",
    "                },p=0.50),\n",
    "                tio.OneOf({\n",
    "                    tio.RandomMotion(num_transforms=3, image_interpolation='nearest') : 0.5,\n",
    "                    tio.RandomBiasField(coefficients=1) : 0.2,\n",
    "                    tio.RandomGhosting(intensity=1.5) : 0.3\n",
    "                }, p=0.50)\n",
    "            ])\n",
    "        }, p=0.8), # randomly apply ONE of these given transforms with prob 0.5 \n",
    "        'val': tio.Compose([\n",
    "            tio.CropOrPad((192, 192, 124)),\n",
    "            tio.EnsureShapeMultiple(2**n_channels, method='pad')\n",
    "        ]),\n",
    "        'test' : tio.Compose([\n",
    "            tio.EnsureShapeMultiple(2**n_channels, method='pad')\n",
    "        ])\n",
    "    }\n",
    "\n",
    "    return data_transforms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define model architecture:\n",
    "        Done before data loader so that transforms has n_channels for EnsureShapeMultiple\n",
    "\"\"\"\n",
    "def define_model(checkpoint=None):\n",
    "    model=UNet(\n",
    "        spatial_dims=3,\n",
    "        in_channels=4,\n",
    "        out_channels=4,\n",
    "        channels=(16, 32, 64, 128, 256),\n",
    "        # channels=(32, 64, 128, 256, 320, 320), #nnunet channels, depth 6\n",
    "        # channels=(64, 96, 128, 192, 256, 384, 512), # optinet, depth 7\n",
    "        strides=(2, 2, 2, 2), # length should = len(channels) - 1\n",
    "        # kernel_size=,\n",
    "        # num_res_units=,\n",
    "        # dropout=0.0,\n",
    "        )\n",
    "    n_channels = len(model.channels)\n",
    "    print(f\"Number of channels: {n_channels}\")\n",
    "\n",
    "    if checkpoint != None:\n",
    "        model.load_state_dict(torch.load(checkpoint, map_location=torch.device('cpu')))\n",
    "\n",
    "    return model, n_channels\n",
    "\n",
    "\"\"\"\n",
    "Setup validation stuff\n",
    "    metrics\n",
    "    post trans ???????\n",
    "    define inference\n",
    "\"\"\"\n",
    "def val_params():\n",
    "    VAL_AMP = True\n",
    "    dice_metric = DiceMetric(include_background=True, reduction=\"mean\", get_not_nans=True, num_classes=4)\n",
    "    dice_metric_batch = DiceMetric(include_background=True, reduction=\"mean_batch\", get_not_nans=True, num_classes=4)\n",
    "    post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
    "    return VAL_AMP, dice_metric, dice_metric_batch, post_trans\n",
    "\n",
    "# define inference method\n",
    "def inference(VAL_AMP, model, input):\n",
    "    def _compute(input):\n",
    "        return sliding_window_inference(\n",
    "            inputs=input,\n",
    "            roi_size=(240, 240, 155),\n",
    "            # roi_size=(128, 128, 64),\n",
    "            sw_batch_size=1,\n",
    "            predictor=model,\n",
    "            overlap=0.5,\n",
    "            mode='gaussian'\n",
    "        )\n",
    "    if VAL_AMP:\n",
    "        with torch.cuda.amp.autocast():\n",
    "            return _compute(input)\n",
    "    else:\n",
    "        return _compute(input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DONT RUN THIS FOR TESTING INFERENCE (TRAIN STUFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Potentially useful functions for model tracking and checkpoint loading\n",
    "\"\"\"\n",
    "# def save_checkpoint(model, epoch, best_acc=0, dir_add=results_dir, args=args):\n",
    "#     filename=f\"chkpt_{args.run_name}_{epoch}_{best_acc}.pt\"\n",
    "#     state_dict = model.state_dict()\n",
    "#     save_dict = {\"epoch\": epoch, \"best_acc\": best_acc, \"state_dict\": state_dict}\n",
    "#     filename = os.path.join(dir_add, filename)\n",
    "#     torch.save(save_dict, filename)\n",
    "#     print(\"\\nSaving checkpoint\", filename)\n",
    "\n",
    "\"\"\"Setup transforms, dataset\"\"\"\n",
    "def define_dataloaders(n_channels):\n",
    "    # Define transforms\n",
    "    data_transform = define_transforms(n_channels)\n",
    "    # Load data\n",
    "    dataloaders = load_data(args, data_transform)                      # this also saves a json splitData\n",
    "    # train_loader, val_loader = dataloaders['train'], dataloaders['val']\n",
    "    return dataloaders\n",
    "\n",
    "\"\"\"Create Model Params:\n",
    "    optimiser\n",
    "    loss fn\n",
    "    lr\n",
    "\"\"\"\n",
    "def model_params(args, model):\n",
    "    # Define optimiser\n",
    "    if args.optimiser == \"adam\":\n",
    "        optimiser = torch.optim.Adam(params=model.parameters(), lr=args.learning_rate)\n",
    "        print(\"Adam optimizer set\")\n",
    "    elif args.optimiser == \"sgd\":\n",
    "        optimiser = torch.optim.SGD(params=model.parameters())\n",
    "        print(\"SGD optimizer set\")\n",
    "    elif args.optimiser == \"novo\":\n",
    "        optimiser = monai.optimizers.Novograd(params=model.parameters(), lr=args.learning_rate)\n",
    "    else:\n",
    "        print(\"Error, no optimiser provided\")\n",
    "\n",
    "    # Define loss function\n",
    "    if args.criterion == \"ce\":\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        print(\"Cross Entropy Loss set\")\n",
    "    elif args.criterion == \"dice\":\n",
    "        criterion = DiceFocalLoss(squared_pred=True, to_onehot_y=False, sigmoid=True)\n",
    "        print(\"Focal-Dice Loss set\")\n",
    "    else:\n",
    "        print(\"Error, no loss fn provided\")\n",
    "\n",
    "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimiser, T_max=args.epochs)\n",
    "    \n",
    "    return optimiser, criterion, lr_scheduler\n",
    "\n",
    "\"\"\"\n",
    "Define training loop\n",
    "    initialise empty lists for val\n",
    "    Add GradScalar which uses automatic mixed precision to accelerate training\n",
    "    forward and backward passes\n",
    "    validate training epoch\n",
    "\n",
    "\"\"\"\n",
    "def train(args, model, device, train_loader, val_loader, optimiser, criterion, lr_scheduler):\n",
    "\n",
    "    VAL_AMP, dice_metric, dice_metric_batch, post_trans = val_params()\n",
    "\n",
    "    # Train model --> see MONAI notebook examples\n",
    "    val_interval = 1\n",
    "    epoch_loss_list = []\n",
    "    val_epoch_loss_list = []\n",
    "\n",
    "    best_metric = -1\n",
    "    best_metric_epoch = -1\n",
    "    best_metrics_epochs_and_time = [[], [], []]\n",
    "\n",
    "    metric_values = []\n",
    "    metric_values_0 = []\n",
    "    metric_values_1 = []\n",
    "    metric_values_2 = []\n",
    "    metric_values_3 = []\n",
    "\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    total_start = time.time()\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        epoch_start = time.time()\n",
    "        # print(\"-\" * 10)\n",
    "        # print(f\"epoch {epoch + 1}/{args.epochs}\")\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), dynamic_ncols=True)\n",
    "        progress_bar.set_description(f\"Training Epoch {epoch}\")\n",
    "\n",
    "        # for step, batch in progress_bar:\n",
    "        for step, batch_data in progress_bar:\n",
    "            step_start = time.time()\n",
    "            inputs, labels = batch_data[0].to(device), batch_data[1].to(device)\n",
    "            optimiser.zero_grad()\n",
    "        \n",
    "            with autocast(): # cast tensor to smaller memory footprint to avoid OOM\n",
    "                \"\"\" FOR USE WITH A DIFFUSION MODEL ONLY\n",
    "                # Generate random noise\n",
    "                noise = torch.randn_like(images).to(device)\n",
    "                Create timesteps\n",
    "                timesteps = torch.randint(\n",
    "                    0, inferer.scheduler.num_train_timesteps, (images.shape[0],), device=images.device\n",
    "                ).long()\n",
    "                # Get model prediction\n",
    "                noise_pred = inferer(inputs=images, diffusion_model=model, noise=noise, timesteps=timesteps)\n",
    "                loss = F.mse_loss(noise_pred.float(), noise.float())\n",
    "                \"\"\"\n",
    "\n",
    "                # print(inputs.shape)\n",
    "                outputs = model(inputs)\n",
    "                # print(outputs.shape)\n",
    "                loss = criterion.forward(outputs, labels)\n",
    "            \n",
    "            # Calculate Loss and Update optimiser using scalar\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimiser)\n",
    "            scaler.update()\n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix({\"bat_train_loss\" : loss.item(), \"Ave_train_loss\" : epoch_loss/(step + 1)})\n",
    "            \n",
    "            print(\n",
    "                f\"\\n{step}/{len(train_loader.dataset)//train_loader.batch_size}\"\n",
    "                f\",     Batch train_loss: {loss.item():.4f}\"\n",
    "                f\",     Step time: {(time.time() - step_start):.4f}\"\n",
    "            )\n",
    "            epoch_loss2 = epoch_loss/(step+1)\n",
    "            lr_scheduler.step()\n",
    "        epoch_loss_list.append(epoch_loss2)\n",
    "        print(f\"\\nEpoch {epoch} average loss: {epoch_loss2:.4f}\")\n",
    "        \n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "            model.eval()\n",
    "            val_epoch_loss = 0\n",
    "            progress_bar = tqdm(enumerate(val_loader), total=len(val_loader),dynamic_ncols=True)\n",
    "            progress_bar.set_description(f\"Val_train Epoch {epoch}\")\n",
    "\n",
    "            for step, batch in enumerate(val_loader):\n",
    "                val_inputs, val_labels = batch[0].to(device), batch[1].to(device)\n",
    "                \"\"\" FOR USE WITH A DIFFUSION MODEL ONLY\n",
    "                timesteps = torch.randint(\n",
    "                    0, inferer.scheduler.num_train_timesteps, (images.shape[0],), device=images.device\n",
    "                ).long()\n",
    "\n",
    "                # Get model prediction\n",
    "                noise_pred = inferer(inputs=images, diffusion_model=model, noise=noise, timesteps=timesteps)\n",
    "                val_loss = F.mse_loss(noise_pred.float(), noise.float())\n",
    "                \"\"\"\n",
    "                with torch.no_grad():\n",
    "                    val_outputs = inference(VAL_AMP, model, val_inputs)\n",
    "                    val_loss = criterion.forward(val_outputs, val_labels)\n",
    "                    \n",
    "                    val_labels_list = decollate_batch(val_labels)\n",
    "                    val_outputs_convert = [post_trans(i) for i in decollate_batch(val_outputs)]\n",
    "                    dice_metric(y_pred=val_outputs_convert, y=val_labels_list)\n",
    "                    dice_metric_batch(y_pred=val_outputs_convert, y=val_labels_list)\n",
    "\n",
    "                val_epoch_loss += val_loss.item()\n",
    "                progress_bar.set_postfix({\"Val_loss\": val_epoch_loss / (step + 1)})\n",
    "            val_epoch_loss_list.append(val_epoch_loss / (step + 1))\n",
    "            \n",
    "            metric = dice_metric.aggregate()[0].item()\n",
    "            metric_values.append(metric)\n",
    "            metric_batch = dice_metric_batch.aggregate()\n",
    "            # print(metric)\n",
    "            # print(metric_batch)\n",
    "\n",
    "            metric_0 = metric_batch[0][0].item()\n",
    "            metric_values_0.append(metric_0)\n",
    "\n",
    "            metric_1 = metric_batch[0][1].item()\n",
    "            metric_values_1.append(metric_1)\n",
    "\n",
    "            metric_2 = metric_batch[0][2].item()\n",
    "            metric_values_2.append(metric_2)\n",
    "\n",
    "            metric_3 = metric_batch[0][3].item()\n",
    "            metric_values_3.append(metric_3)\n",
    "\n",
    "            dice_metric.reset()\n",
    "            dice_metric_batch.reset()\n",
    "\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                best_metrics_epochs_and_time[0].append(best_metric)\n",
    "                best_metrics_epochs_and_time[1].append(best_metric_epoch)\n",
    "                best_metrics_epochs_and_time[2].append(time.time() - total_start)\n",
    "                save_checkpoint(\n",
    "                        model,\n",
    "                        epoch,\n",
    "                        best_acc=best_metric,\n",
    "                    )\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    os.path.join(args.result, f\"best_metric_model_{args.run_name}.pth\"),\n",
    "                )\n",
    "                print(\"\\nsaved new best metric model\")\n",
    "            print(\n",
    "                f\"\\ncurrent epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
    "                f\"\\nMean Dice per Region is: label 1: {metric_1:.4f};  label 2: {metric_2:.4f} label 3: {metric_3:.4f}\"\n",
    "                f\"\\nbest mean dice: {best_metric:.4f}\"\n",
    "                f\" at epoch: {best_metric_epoch}\"\n",
    "            )\n",
    "        print(f\"time consuming of epoch {epoch + 1} is: {(time.time() - epoch_start):.4f}\")\n",
    "    total_time = time.time() - total_start"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of channels: 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (model): Sequential(\n",
       "    (0): Convolution(\n",
       "      (conv): Conv3d(4, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "      (adn): ADN(\n",
       "        (N): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (D): Dropout(p=0.0, inplace=False)\n",
       "        (A): PReLU(num_parameters=1)\n",
       "      )\n",
       "    )\n",
       "    (1): SkipConnection(\n",
       "      (submodule): Sequential(\n",
       "        (0): Convolution(\n",
       "          (conv): Conv3d(64, 96, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "          (adn): ADN(\n",
       "            (N): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (D): Dropout(p=0.0, inplace=False)\n",
       "            (A): PReLU(num_parameters=1)\n",
       "          )\n",
       "        )\n",
       "        (1): SkipConnection(\n",
       "          (submodule): Sequential(\n",
       "            (0): Convolution(\n",
       "              (conv): Conv3d(96, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                (D): Dropout(p=0.0, inplace=False)\n",
       "                (A): PReLU(num_parameters=1)\n",
       "              )\n",
       "            )\n",
       "            (1): SkipConnection(\n",
       "              (submodule): Sequential(\n",
       "                (0): Convolution(\n",
       "                  (conv): Conv3d(128, 192, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "                  (adn): ADN(\n",
       "                    (N): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                    (D): Dropout(p=0.0, inplace=False)\n",
       "                    (A): PReLU(num_parameters=1)\n",
       "                  )\n",
       "                )\n",
       "                (1): SkipConnection(\n",
       "                  (submodule): Sequential(\n",
       "                    (0): Convolution(\n",
       "                      (conv): Conv3d(192, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "                      (adn): ADN(\n",
       "                        (N): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                        (D): Dropout(p=0.0, inplace=False)\n",
       "                        (A): PReLU(num_parameters=1)\n",
       "                      )\n",
       "                    )\n",
       "                    (1): SkipConnection(\n",
       "                      (submodule): Sequential(\n",
       "                        (0): Convolution(\n",
       "                          (conv): Conv3d(256, 384, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "                          (adn): ADN(\n",
       "                            (N): InstanceNorm3d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                            (D): Dropout(p=0.0, inplace=False)\n",
       "                            (A): PReLU(num_parameters=1)\n",
       "                          )\n",
       "                        )\n",
       "                        (1): SkipConnection(\n",
       "                          (submodule): Convolution(\n",
       "                            (conv): Conv3d(384, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "                            (adn): ADN(\n",
       "                              (N): InstanceNorm3d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                              (D): Dropout(p=0.0, inplace=False)\n",
       "                              (A): PReLU(num_parameters=1)\n",
       "                            )\n",
       "                          )\n",
       "                        )\n",
       "                        (2): Convolution(\n",
       "                          (conv): ConvTranspose3d(896, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n",
       "                          (adn): ADN(\n",
       "                            (N): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                            (D): Dropout(p=0.0, inplace=False)\n",
       "                            (A): PReLU(num_parameters=1)\n",
       "                          )\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                    (2): Convolution(\n",
       "                      (conv): ConvTranspose3d(512, 192, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n",
       "                      (adn): ADN(\n",
       "                        (N): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                        (D): Dropout(p=0.0, inplace=False)\n",
       "                        (A): PReLU(num_parameters=1)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (2): Convolution(\n",
       "                  (conv): ConvTranspose3d(384, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n",
       "                  (adn): ADN(\n",
       "                    (N): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                    (D): Dropout(p=0.0, inplace=False)\n",
       "                    (A): PReLU(num_parameters=1)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): Convolution(\n",
       "              (conv): ConvTranspose3d(256, 96, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                (D): Dropout(p=0.0, inplace=False)\n",
       "                (A): PReLU(num_parameters=1)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): Convolution(\n",
       "          (conv): ConvTranspose3d(192, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n",
       "          (adn): ADN(\n",
       "            (N): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (D): Dropout(p=0.0, inplace=False)\n",
       "            (A): PReLU(num_parameters=1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Convolution(\n",
       "      (conv): ConvTranspose3d(128, 3, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"General Setup: \n",
    "    logging,\n",
    "    utils.args \n",
    "    seed,\n",
    "    cuda, \n",
    "    root dir\"\"\"\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "# args = get_main_args()\n",
    "seed = 42\n",
    "set_determinism(seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# root_dir = args.data\n",
    "# results_dir = args.results\n",
    "\n",
    "model, n_channels = define_model()\n",
    "model.to(device)\n",
    "\n",
    "# Print out model architecture\n",
    "# print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = define_dataloaders(n_channels)\n",
    "train_loader, val_loader = dataloaders['train'], dataloaders['val']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser, criterion, lr_scheduler = model_params(args, model)\n",
    "\n",
    "# TRAIN MODEL\n",
    "train(args, model, device, train_loader, val_loader, optimiser, criterion, lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"red\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val Mean Dice\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(\"train\", (18, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Val Mean Dice TC\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values_tc))]\n",
    "y = metric_values_tc\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"blue\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Val Mean Dice WT\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values_wt))]\n",
    "y = metric_values_wt\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"brown\")\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Val Mean Dice ET\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values_et))]\n",
    "y = metric_values_et\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"purple\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN FROM HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File paths for Alex's local system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dir= '/Users/alexandrasmith/Desktop/Workspace/Projects/UNN_BraTS23/data/val_SSA'\n",
    "validation_files = [os.path.join(validation_dir, file) for file in os.listdir(validation_dir)]\n",
    "\n",
    "# checkpoint to test: /scratch/guest187/Data/train_all/results/test_run/best_metric_model.pth \n",
    "checkpoint = '/Users/alexandrasmith/Desktop/Workspace/Projects/UNN_BraTS23/data/best_metric_model_fullTest.pth'\n",
    "\n",
    "# Load validation data to dataloader\n",
    "data_transforms = define_transforms(n_channels)\n",
    "validation_dataset = MRIDataset(validation_dir, validation_files, transform=data_transforms['val'])\n",
    "\n",
    "# Validation parameters\n",
    "VAL_AMP, dice_metric, dice_metric_batch, post_transforms = val_params()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check best model output with the input image and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "tranformed_subject:  Subject(Keys: ('image',); images: 1)\n",
      "torch.Size([4, 192, 192, 128])\n"
     ]
    }
   ],
   "source": [
    "print(len(validation_dataset))\n",
    "img, image_path = validation_dataset[0]\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of channels: 5\n",
      "tranformed_subject:  Subject(Keys: ('image',); images: 1)\n",
      "torch.Size([4, 192, 192, 128])\n",
      "torch.Size([4, 192, 192, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandrasmith/miniforge3/envs/spark/lib/python3.9/site-packages/torch/amp/autocast_mode.py:221: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Sequence must have length 2, got 3.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(val_input\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      9\u001b[0m \u001b[39m# roi_size = (128, 128, 64)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m# sw_batch_size = 4\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m val_output \u001b[39m=\u001b[39m inference(VAL_AMP, model, val_input)\n\u001b[1;32m     12\u001b[0m val_output \u001b[39m=\u001b[39m post_transforms(val_output[\u001b[39m0\u001b[39m])\n\u001b[1;32m     13\u001b[0m plt\u001b[39m.\u001b[39mfigure(\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m, (\u001b[39m24\u001b[39m, \u001b[39m6\u001b[39m))\n",
      "Cell \u001b[0;32mIn[78], line 53\u001b[0m, in \u001b[0;36minference\u001b[0;34m(VAL_AMP, model, input)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mif\u001b[39;00m VAL_AMP:\n\u001b[1;32m     52\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast():\n\u001b[0;32m---> 53\u001b[0m         \u001b[39mreturn\u001b[39;00m _compute(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m     54\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     \u001b[39mreturn\u001b[39;00m _compute(\u001b[39minput\u001b[39m)\n",
      "Cell \u001b[0;32mIn[78], line 42\u001b[0m, in \u001b[0;36minference.<locals>._compute\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_compute\u001b[39m(\u001b[39minput\u001b[39m):\n\u001b[1;32m     41\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m sliding_window_inference(\n\u001b[1;32m     43\u001b[0m         inputs\u001b[39m=\u001b[39;49m\u001b[39minput\u001b[39;49m,\n\u001b[1;32m     44\u001b[0m         roi_size\u001b[39m=\u001b[39;49m(\u001b[39m240\u001b[39;49m, \u001b[39m240\u001b[39;49m, \u001b[39m155\u001b[39;49m),\n\u001b[1;32m     45\u001b[0m         \u001b[39m# roi_size=(128, 128, 64),\u001b[39;49;00m\n\u001b[1;32m     46\u001b[0m         sw_batch_size\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     47\u001b[0m         predictor\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     48\u001b[0m         overlap\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m,\n\u001b[1;32m     49\u001b[0m         mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgaussian\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[1;32m     50\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/spark/lib/python3.9/site-packages/monai/inferers/utils.py:158\u001b[0m, in \u001b[0;36msliding_window_inference\u001b[0;34m(inputs, roi_size, sw_batch_size, predictor, overlap, mode, sigma_scale, padding_mode, cval, sw_device, device, progress, roi_weight_map, process_fn, buffer_steps, buffer_dim, *args, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m     temp_meta \u001b[39m=\u001b[39m MetaTensor([])\u001b[39m.\u001b[39mcopy_meta_from(inputs, copy_attr\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    157\u001b[0m inputs \u001b[39m=\u001b[39m convert_data_type(inputs, torch\u001b[39m.\u001b[39mTensor, wrap_sequence\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m--> 158\u001b[0m roi_size \u001b[39m=\u001b[39m fall_back_tuple(roi_size, image_size_)\n\u001b[1;32m    160\u001b[0m \u001b[39m# in case that image size is smaller than roi size\u001b[39;00m\n\u001b[1;32m    161\u001b[0m image_size \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\u001b[39mmax\u001b[39m(image_size_[i], roi_size[i]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_spatial_dims))\n",
      "File \u001b[0;32m~/miniforge3/envs/spark/lib/python3.9/site-packages/monai/utils/misc.py:276\u001b[0m, in \u001b[0;36mfall_back_tuple\u001b[0;34m(user_provided, default, func)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[39mRefine `user_provided` according to the `default`, and returns as a validated tuple.\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    273\u001b[0m \n\u001b[1;32m    274\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    275\u001b[0m ndim \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(default)\n\u001b[0;32m--> 276\u001b[0m user \u001b[39m=\u001b[39m ensure_tuple_rep(user_provided, ndim)\n\u001b[1;32m    277\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(  \u001b[39m# use the default values if user provided is not valid\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     user_c \u001b[39mif\u001b[39;00m func(user_c) \u001b[39melse\u001b[39;00m default_c \u001b[39mfor\u001b[39;00m default_c, user_c \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(default, user)\n\u001b[1;32m    279\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/spark/lib/python3.9/site-packages/monai/utils/misc.py:202\u001b[0m, in \u001b[0;36mensure_tuple_rep\u001b[0;34m(tup, dim)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(tup) \u001b[39m==\u001b[39m dim:\n\u001b[1;32m    200\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(tup)\n\u001b[0;32m--> 202\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSequence must have length \u001b[39m\u001b[39m{\u001b[39;00mdim\u001b[39m}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(tup)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Sequence must have length 2, got 3."
     ]
    }
   ],
   "source": [
    "model, n_channels = define_model(checkpoint)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # select one image to evaluate and visualize the model output\n",
    "    val_input, _ = validation_dataset[0]\n",
    "    print(val_input.shape)\n",
    "    # roi_size = (128, 128, 64)\n",
    "    # sw_batch_size = 4\n",
    "    val_output = inference(VAL_AMP, model, val_input)\n",
    "    val_output = post_transforms(val_output[0])\n",
    "    plt.figure(\"image\", (24, 6))\n",
    "    for i in range(4):\n",
    "        plt.subplot(1, 4, i + 1)\n",
    "        plt.title(f\"image channel {i}\")\n",
    "        plt.imshow(validation_dataset[6][\"image\"][i, :, :, 70].detach().cpu(), cmap=\"gray\")\n",
    "    plt.show()\n",
    "    # visualize the 3 channels label corresponding to this image\n",
    "    plt.figure(\"label\", (18, 6))\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        plt.title(f\"label channel {i}\")\n",
    "        plt.imshow(validation_dataset[6][\"label\"][i, :, :, 70].detach().cpu())\n",
    "    plt.show()\n",
    "    # visualize the 3 channels model output corresponding to this image\n",
    "    plt.figure(\"output\", (18, 6))\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        plt.title(f\"output channel {i}\")\n",
    "        plt.imshow(val_output[i, :, :, 70].detach().cpu())\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

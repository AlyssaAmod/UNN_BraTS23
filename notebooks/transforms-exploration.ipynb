{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentations Check"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing:\n",
    "Here we will check what effects torchio transformations have on each volume\n",
    "1. Crop or Pad\n",
    "2. Mask - normalisation using label as a mask\n",
    "3. One Hot Encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data using the data_prep code\n",
    "**NB: DO NOT RUN THE DATA PREPROC PART YET**\n",
    "\n",
    "To ensure we are just working with a few functions at a time, py script is copied in below without preproc. We will run each example on several SSA and GLI datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some functions from scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import os\n",
    "import random\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import json\n",
    "from glob import glob\n",
    "import time\n",
    "import numpy as np\n",
    "from matplotlib import rcParams \n",
    "import torchio as tio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data files\n",
    "\n",
    "# CHANGE data_dir path as per your local set up\n",
    "pthAlex = '/Users/alexandrasmith/Desktop/Workspace/Projects/UNN_BraTS23/data/ASNR-MICCAI-BraTS2023-SSA-Challenge-TrainingData/'\n",
    "pthAly = 'C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\Data\\\\Samples\\\\'\n",
    "\n",
    "data_dir2 = pthAly\n",
    "print(data_dir2)\n",
    "specific_string = 'BraTS-'\n",
    "# folders = os.listdir(data_dir)\n",
    "folders = [folder for folder in os.listdir(data_dir2) if os.path.isdir(os.path.join(data_dir2, folder)) and folder.startswith(specific_string)]\n",
    "\n",
    "print(\"Total folders: \", len(folders), \"\\n Subjects: \",  folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select one subject to Explore\n",
    "data_dir2 = pthAly\n",
    "print(data_dir2)\n",
    "specific_string = 'BraTS-'\n",
    "# folders = os.listdir(data_dir)\n",
    "folders = [folder for folder in os.listdir(data_dir2) if os.path.isdir(os.path.join(data_dir2, folder)) and folder.startswith(specific_string)]\n",
    "\n",
    "print(\"Total folders: \", len(folders), \"\\n Subjects: \",  folders)\n",
    "#randomly select a subject\n",
    "img_folder = folders[random.randrange(0, len(folders))]\n",
    "print(f\"Working with subject: {img_folder}\")\n",
    "\n",
    "## Set up files\n",
    "# Load image volumes\n",
    "img_volumes = [nib.load(os.path.join(data_dir2 + img_folder, img_folder + f\"-{m}.nii.gz\")) for m in [\"t1c\", \"t1n\", \"t2f\", \"t2w\"]]\n",
    "\n",
    "# Load segmentation volume\n",
    "seg_volume = nib.load(os.path.join(data_dir2 + img_folder, img_folder + \"-seg.nii.gz\"))\n",
    "\n",
    "subjects = [subj for subj in folders] \n",
    "print(subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_tio = []\n",
    "for s in subjects:\n",
    "        subject = tio.Subject(\n",
    "                image=tio.ScalarImage(os.path.join(data_dir2 + s, s + \"-t1n.nii.gz\")),\n",
    "                label=tio.LabelMap(os.path.join(data_dir2 + s, s + \"-seg.nii.gz\")))\n",
    "        subjects_tio.append(subject)\n",
    "print(subjects_tio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "# Define the original array\n",
    "original_array = np.random.random((240, 240, 155))\n",
    "\n",
    "# Define the scaling factor\n",
    "scale_factor = 0.8\n",
    "\n",
    "# Calculate the new dimensions\n",
    "new_shape = tuple(int(dim * scale_factor) for dim in original_array.shape)\n",
    "\n",
    "# Scale down the array\n",
    "scaled_array = zoom(original_array, scale_factor)\n",
    "\n",
    "# Verify the new shape\n",
    "print(\"Original shape:\", original_array.shape)\n",
    "print(\"Scaled shape:\", scaled_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_ras = tio.ToCanonical()\n",
    "crop_pad_Zein = tio.CropOrPad((192, 224, 160), mask_name=\"label\")\n",
    "crop_padZ = tio.CropOrPad((192, 224, 160))\n",
    "crop_padSc = tio.CropOrPad((192, 192, 124))\n",
    "crop_pad_lbl = tio.CropOrPad(mask_name=\"label\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in subjects_tio:\n",
    "    ras = to_ras(s)\n",
    "    ras.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in subjects_tio:\n",
    "    cpSc = crop_padSc(s)\n",
    "    cpZ = crop_padZ(s)\n",
    "    cpSc.plot()\n",
    "    cpZ.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in subjects_tio:\n",
    "    cpZ = crop_pad_Zein(s)\n",
    "    cpZ.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in subjects_tio:\n",
    "    cpl = crop_pad_lbl(s)\n",
    "    cpl.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FULL VISUALISATION -- USE EPS VERSION AT END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pthAly = 'C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\Data\\\\BraTS2023-SSA-TrainingData\\\\'\n",
    "\n",
    "data_dir2 = pthAly\n",
    "print(data_dir2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torchio as tio\n",
    "\n",
    "data_dir = 'C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\Data\\\\BraTS2023-SSA-TrainingData\\\\'\n",
    "output_dir = 'C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\Data\\\\BraTS2023-SSA-TrainingData\\\\output'\n",
    "\n",
    "modalities = ['t1n', 't2w', 't1c', 't2f', 'seg']\n",
    "views = ['axial', 'sagittal', 'coronal']\n",
    "\n",
    "transformations = [\n",
    "    tio.ToCanonical(),\n",
    "    tio.CropOrPad((192, 224, 160), mask_name=\"label\"),\n",
    "    tio.CropOrPad((192, 224, 160)),\n",
    "    tio.CropOrPad((192, 192, 124)),\n",
    "    tio.CropOrPad(mask_name=\"label\"),\n",
    "    tio.RandomFlip(axes=(0, 1, 2), p=0.3),\n",
    "    tio.Resample((1.2, 1.2, 6)),\n",
    "    tio.RandomAnisotropy(axes=(0, 1, 2), downsampling=(1, 6)),\n",
    "    tio.RandomBlur(std=(0.5, 1.5)),\n",
    "    tio.RandomNoise(mean=0, std=(0, 0.33)),\n",
    "    tio.RandomMotion(num_transforms=3, image_interpolation='nearest'),\n",
    "    tio.RandomBiasField(coefficients=1),\n",
    "    tio.RandomGhosting(intensity=1.5)\n",
    "]\n",
    "\n",
    "specific_string = 'BraTS-'\n",
    "folders = [folder for folder in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, folder)) and folder.startswith(specific_string)]\n",
    "subjects = [subj for subj in folders]\n",
    "\n",
    "subjects_tio = []\n",
    "for subject in subjects:\n",
    "    subject_dir = os.path.join(data_dir, subject)\n",
    "    subject_images = {}\n",
    "    \n",
    "    for modality in modalities:\n",
    "        file_pth = os.path.join(subject_dir, f'{subject}-{modality}.nii.gz')\n",
    "        \n",
    "        if modality == 'seg' and file_pth in seg_p:\n",
    "            subject_images[\"label\"] = tio.LabelMap(file_pth)\n",
    "        else:\n",
    "            subject_images[modality] = tio.ScalarImage(file_pth)\n",
    "    \n",
    "    subject_tio = tio.Subject(**subject_images)\n",
    "    subjects_tio.append(subject_tio)\n",
    "\n",
    "print(subjects_tio)\n",
    "\n",
    "for modality in modalities:\n",
    "    for subject in subjects_tio:\n",
    "        fig, axes = plt.subplots(len(views), len(transformations), figsize=(20, 20))\n",
    "        fig.suptitle(f'{subject} - {modality.capitalize()} Transforms')\n",
    "\n",
    "        for i, view in enumerate(views):\n",
    "            for j, transformation in enumerate(transformations):\n",
    "                transformed_image = transformation(subject)\n",
    "\n",
    "                if view == 'axial':\n",
    "                    img = transformed_image.plot(axial=True)\n",
    "                elif view == 'sagittal':\n",
    "                    img = transformed_image.plot(sagittal=True)\n",
    "                elif view == 'coronal':\n",
    "                    img = transformed_image.plot(coronal=True)\n",
    "\n",
    "                axes[i, j].imshow(img, cmap='gray')\n",
    "                axes[i, j].axis('off')\n",
    "\n",
    "                # Save figure as .png file\n",
    "                filename = f'{modality}_{view}_{j+1}.png'\n",
    "                plt.savefig(os.path.join(output_dir, filename))\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amoda\\Documents\\SPARK\\BraTS2023\\Data\\BraTS2023-SSA-TrainingData\\BraTS-SSA-00002-000\n",
      "C:\\Users\\amoda\\Documents\\SPARK\\BraTS2023\\Data\\BraTS2023-SSA-TrainingData\\BraTS-SSA-00007-000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 65\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m current_transformation \u001b[39min\u001b[39;00m transformations:\n\u001b[0;32m     64\u001b[0m         \u001b[39mfor\u001b[39;00m current_subject \u001b[39min\u001b[39;00m dataset:\n\u001b[1;32m---> 65\u001b[0m             transformed_subject \u001b[39m=\u001b[39m current_transformation(current_subject)\n\u001b[0;32m     66\u001b[0m             transformed_subjects\u001b[39m.\u001b[39mappend(transformed_subject)\n\u001b[0;32m     68\u001b[0m transformed_dataset \u001b[39m=\u001b[39m tio\u001b[39m.\u001b[39mSubjectsDataset(transformed_subjects)\n",
      "File \u001b[1;32mc:\\Users\\amoda\\anaconda3\\lib\\site-packages\\torchio\\transforms\\transform.py:163\u001b[0m, in \u001b[0;36mTransform.__call__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    161\u001b[0m     subject \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mcopy(subject)\n\u001b[0;32m    162\u001b[0m \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39merrstate(\u001b[39mall\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m'\u001b[39m, under\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 163\u001b[0m     transformed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_transform(subject)\n\u001b[0;32m    164\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39mfor\u001b[39;00m name, image \u001b[39min\u001b[39;00m images_to_keep\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\Users\\amoda\\anaconda3\\lib\\site-packages\\torchio\\transforms\\augmentation\\intensity\\random_ghosting.py:105\u001b[0m, in \u001b[0;36mRandomGhosting.apply_transform\u001b[1;34m(self, subject)\u001b[0m\n\u001b[0;32m    103\u001b[0m     arguments[\u001b[39m'\u001b[39m\u001b[39mrestore\u001b[39m\u001b[39m'\u001b[39m][name] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestore\n\u001b[0;32m    104\u001b[0m transform \u001b[39m=\u001b[39m Ghosting(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_include_exclude(arguments))\n\u001b[1;32m--> 105\u001b[0m transformed \u001b[39m=\u001b[39m transform(subject)\n\u001b[0;32m    106\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(transformed, Subject)\n\u001b[0;32m    107\u001b[0m \u001b[39mreturn\u001b[39;00m transformed\n",
      "File \u001b[1;32mc:\\Users\\amoda\\anaconda3\\lib\\site-packages\\torchio\\transforms\\transform.py:163\u001b[0m, in \u001b[0;36mTransform.__call__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    161\u001b[0m     subject \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mcopy(subject)\n\u001b[0;32m    162\u001b[0m \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39merrstate(\u001b[39mall\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m'\u001b[39m, under\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 163\u001b[0m     transformed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_transform(subject)\n\u001b[0;32m    164\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39mfor\u001b[39;00m name, image \u001b[39min\u001b[39;00m images_to_keep\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\Users\\amoda\\anaconda3\\lib\\site-packages\\torchio\\transforms\\augmentation\\intensity\\random_ghosting.py:186\u001b[0m, in \u001b[0;36mGhosting.apply_transform\u001b[1;34m(self, subject)\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(intensity, \u001b[39mfloat\u001b[39m)\n\u001b[0;32m    185\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(restore, \u001b[39mfloat\u001b[39m)\n\u001b[1;32m--> 186\u001b[0m     transformed_tensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_artifact(\n\u001b[0;32m    187\u001b[0m         tensor,\n\u001b[0;32m    188\u001b[0m         num_ghosts,\n\u001b[0;32m    189\u001b[0m         axis,\n\u001b[0;32m    190\u001b[0m         intensity,\n\u001b[0;32m    191\u001b[0m         restore,\n\u001b[0;32m    192\u001b[0m     )\n\u001b[0;32m    193\u001b[0m     transformed_tensors\u001b[39m.\u001b[39mappend(transformed_tensor)\n\u001b[0;32m    194\u001b[0m image\u001b[39m.\u001b[39mset_data(torch\u001b[39m.\u001b[39mstack(transformed_tensors))\n",
      "File \u001b[1;32mc:\\Users\\amoda\\anaconda3\\lib\\site-packages\\torchio\\transforms\\augmentation\\intensity\\random_ghosting.py:208\u001b[0m, in \u001b[0;36mGhosting.add_artifact\u001b[1;34m(self, tensor, num_ghosts, axis, intensity, restore_center)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m num_ghosts \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m intensity:\n\u001b[0;32m    206\u001b[0m     \u001b[39mreturn\u001b[39;00m tensor\n\u001b[1;32m--> 208\u001b[0m spectrum \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfourier_transform(tensor)\n\u001b[0;32m    210\u001b[0m shape \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(tensor\u001b[39m.\u001b[39mshape)\n\u001b[0;32m    211\u001b[0m ri, rj, rk \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mround(restore_center \u001b[39m*\u001b[39m shape)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39muint16)\n",
      "File \u001b[1;32mc:\\Users\\amoda\\anaconda3\\lib\\site-packages\\torchio\\transforms\\fourier.py:11\u001b[0m, in \u001b[0;36mFourierTransform.fourier_transform\u001b[1;34m(tensor)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfft\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     transformed \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mfft\u001b[39m.\u001b[39;49mfftn(tensor)\n\u001b[0;32m     12\u001b[0m     fshift \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfft\u001b[39m.\u001b[39mfftshift(transformed)\n\u001b[0;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m fshift\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchio as tio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the main data directory path\n",
    "data_dir = 'C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\Data\\\\BraTS2023-SSA-TrainingData\\\\'\n",
    "output_dir = os.path.join(data_dir, 'output')\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define the list of transformations to apply\n",
    "transformations = [\n",
    "    tio.ToCanonical(),\n",
    "    tio.CropOrPad((192, 224, 160), mask_name=\"seg\"),\n",
    "    tio.CropOrPad((192, 224, 160)),\n",
    "    tio.CropOrPad((192, 192, 124)),\n",
    "    tio.CropOrPad(mask_name=\"seg\"),\n",
    "    tio.RandomFlip(axes=(0, 1, 2), p=0.3),\n",
    "    tio.Resample((1.2, 1.2, 6)),\n",
    "    tio.RandomAnisotropy(axes=(0, 1, 2), downsampling=(1, 6)),\n",
    "    tio.RandomBlur(std=(0.5, 1.5)),\n",
    "    tio.RandomNoise(mean=0, std=(0, 0.33)),\n",
    "    tio.RandomMotion(num_transforms=3, image_interpolation='nearest'),\n",
    "    tio.RandomBiasField(coefficients=1),\n",
    "    tio.RandomGhosting(intensity=1.5)\n",
    "]\n",
    "\n",
    "# Iterate through the subject folders\n",
    "subject_dirs = sorted([os.path.join(data_dir, subject_dir) for subject_dir in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, subject_dir))])\n",
    "for subject_dir in subject_dirs:\n",
    "    subject_id = os.path.basename(subject_dir)\n",
    "    print(subject_dir)\n",
    "    # Create a list to hold the subject's images and labels\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    # Load each scan modality and segmentation\n",
    "    for modality in ['t1n', 't1c', 't2w', 't2f']:\n",
    "        image_path = os.path.join(subject_dir, f'{subject_id}-{modality}.nii.gz')\n",
    "        image = tio.ScalarImage(image_path)\n",
    "        images.append(image)\n",
    "\n",
    "    label_path = os.path.join(subject_dir, f'{subject_id}-seg.nii.gz')\n",
    "    label = tio.LabelMap(label_path)\n",
    "    labels.append(label)\n",
    "\n",
    "    # Create the subject using the images and labels\n",
    "    subject = tio.Subject(\n",
    "        t1n=images[0],\n",
    "        t1c=images[1],\n",
    "        t2w=images[2],\n",
    "        t2f=images[3],\n",
    "        seg=labels[0]\n",
    "    )\n",
    "\n",
    "    # Create the dataset with the subject\n",
    "    dataset = tio.SubjectsDataset([subject])\n",
    "\n",
    "    # Apply transformations and save the resulting figures\n",
    "    transformed_subjects = []\n",
    "    for current_transformation in transformations:\n",
    "        for current_subject in dataset:\n",
    "            transformed_subject = current_transformation(current_subject)\n",
    "            transformed_subjects.append(transformed_subject)\n",
    "\n",
    "transformed_dataset = tio.SubjectsDataset(transformed_subjects)\n",
    "\n",
    "# Create a figure for all views\n",
    "fig, axes = plt.subplots(len(subject_dirs), 4, figsize=(12, 3 * len(subject_dirs)))\n",
    "\n",
    "# Iterate through the transformed subjects and plot the images\n",
    "for subject_index, transformed_subject in enumerate(transformed_dataset):\n",
    "    # Get the transformed images and labels\n",
    "    transformed_images = [transformed_subject['t1n'], transformed_subject['t1c'], transformed_subject['t2w'], transformed_subject['t2f']]\n",
    "    transformed_label = transformed_subject['seg']\n",
    "\n",
    "    # Iterate through the views and plot the images\n",
    "    for j, view in enumerate(['axial', 'coronal', 'sagittal']):\n",
    "        ax = axes[subject_index, j]\n",
    "        ax.imshow(transformed_images[j].data.squeeze().numpy()[:,:,view])\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'{modality.upper()} {view}')\n",
    "\n",
    "    # Plot the segmentation label for the current subject\n",
    "    ax = axes[subject_index, 3]\n",
    "    ax.imshow(transformed_label.data.squeeze().numpy()[:,:,view])\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Segmentation')\n",
    "\n",
    "# Save the figure as an EPS file\n",
    "output_filename = f'all_subjects_{view}_{transformation.name}.eps'\n",
    "output_path = os.path.join(output_dir, output_filename)\n",
    "plt.savefig(output_path, format='eps')\n",
    "\n",
    "plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
